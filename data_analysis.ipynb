{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.colors as color\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "#Library for additional statistics and the descriptive function summary_cont() (more info than stats.describe()) \n",
    "import researchpy as rp\n",
    "#Module to implement ANOVA tests\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "#Module to implement post-hoc tests with Tukey's HSD method\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16, 7)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the data structure: \n",
    "Our main dataset [Dunnhumby - The complete journey](https://www.dunnhumby.com/careers/engineering/sourcefiles) comprises 8 csv files with the following tabular structure and content.\n",
    "\n",
    "#### Products\n",
    "Product Id | Manufacturer Id | Departement  | Brand | Commodity description | Sub commodity description | Size\n",
    "--- | --- | --- | --- | --- | --- | --- \n",
    " | | {grocery, pastry, etc.} | {national/private} | | |\n",
    "\n",
    "\n",
    "#### Campaign Desc\n",
    "> This table gives the length of time for which a campaign runs. So, any coupons received as part of a campaign are valid within the dates contained in this table.\n",
    "\n",
    "Description | Campaign ID | Start and End Day\n",
    "--- | --- | ---\n",
    "{TypeA, TypeB, TypeC} | int | int (probably day count from study)\n",
    "\n",
    "\n",
    "#### Campaign Table\n",
    "> This table lists the campaigns received by each household in the study. Each household received a different set of campaigns.\n",
    "\n",
    "Description | Campaign ID | Household key\n",
    "--- | --- | ---\n",
    "{TypeA, TypeB, TypeC} | int | int\n",
    "\n",
    "\n",
    "#### Coupon Redemption\n",
    "Household ID | Day | Coupon ID | Campaign ID \n",
    "--- | --- | --- | --- \n",
    "int | int | int | int \n",
    "\n",
    "\n",
    "#### Coupon\n",
    "> This table lists all the coupons sent to customers as part of a campaign, as well as the products for which each coupon is redeemable. Some coupons are redeemable for multiple products. \n",
    "\n",
    "Coupon ID | Product ID | Campaign ID \n",
    "--- | --- | ---  \n",
    "int | int | int \n",
    "\n",
    "\n",
    "#### Casual Data\n",
    "> This table signifies whether a given product was featured in the weekly mailer or was part of an in-store display (other than regular product placement).\n",
    "\n",
    "Product ID | Store ID | Week | Display location | Mailer location\n",
    "--- | --- | --- | --- | --- \n",
    "int | int | int | int | String\n",
    " | | | Advertisement in in-store display? | Featured as ad in weekly mailer\n",
    "\n",
    "\n",
    "#### Demographic\n",
    "> This table contains demographic information for a portion of households. Due to nature of the data, the demographic information is not available for all households.\n",
    "\n",
    "Age | Marital Status Code | Income | Homeowner | Household composition | Household size | Number of kids | Household Id\n",
    "--- | --- | --- | --- | --- | --- | --- | --- |\n",
    "(19-65+) | {A: Married, B: Single, U: Unknown} | | {Homeowner, Retired, etc.} | {Female/Male single, adults with/without kids, etc.} || (1-3+) |\n",
    " \n",
    " \n",
    "### Transaction\n",
    "> This table contains all products purchased by households within this study. Each line found in this table is essentially the same line that would be found on a store receipt.\n",
    "\n",
    "Household Id | Manufacturer Id | Week | Day | Time of Day | Product Id | Quantity | Sales value | Store Id | Retail discount | Coupon discount | Coupon match discount\n",
    "--- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | \n",
    " | | | | | | | What the shop actually gets | | | Loyalty program of retailer | Shop does not get price before discount | Shop does get price before discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:30px 30px; background-color: #3388FF; text-align: center; font-size:30px;\">\n",
    "   <strong>Exploring the data</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first section is to explore and get a sense of the data. Let's start by loading it in pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Products\n",
    "The first thing we want to do is design meaningful categories for the product we have in our database. This will allow us to study consumptions behaviours of the households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = pd.read_csv('dunnhumby/product.csv')\n",
    "df_products.columns = map(str.lower, df_products.columns)\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the product data frame and filter out anything food related. We look at the columns *department*, *commodity_desc* and *sub_commodity_desc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df, word_list):\n",
    "    filtered_df = df.copy()\n",
    "    for word in word_list:\n",
    "        filtered_df = filtered_df[~(filtered_df[\"department\"].str.contains(word) | filtered_df[\"commodity_desc\"].str.contains(word) | filtered_df[\"sub_commodity_desc\"].str.contains(word))]\n",
    "    return df[~df.index.isin(filtered_df.index)], df[df.index.isin(filtered_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meat_list = [\"MEAT\",\"PORK\", \"BEEF\", \"DUCK\", \"CHICKEN\", \"POULTRY\", \"LAMB\", \"VEAL\",\n",
    "             \"MUTTON\", \"TURKEY\", \"VENISON\", \"WILD BOAR\", \"BISON\", \"GOOSE\", \"RABBIT\", \"PHEASANT\"]\n",
    "seafood_list = [\"HERRING\", \"SEAFOOD\", \"SEAFD\",\"SUSHI\", \"FISH\",\"SHRIMP\",\"SALMON\",\"TUNA\",\n",
    "                \"TILAPIA\",\"ALASKA\",\"POLLOCK\",\"PANGASIUS\",\"BASA\",\"SWAI\",\"COD\",\"CATFISH\",\"CRAB\",\"CLAMS\"]\n",
    "sweets_snack_list = [\"ROLLS\", \"GUM\", \"PASTRY\", \"NUT\",\"CRACKERS\",\"DESSERT\",\"SNKS\",\"SNACK\",\"CHIP\",\n",
    "                     \"CANDY\", \"ICE\", \"SWEET\", \"CHOCOLATE\",\"POPCORN\", \"CAKE\", \"COOKIE\", \"PANCAKE\",\n",
    "                     \"BAKING\", \"BAKE\", \"PIE\"]\n",
    "fruits_list = [\"FRUIT\",\"APPLE\", \"ORANGE\", \"PEAR\", \"BANANA\", \"GRAPE\", \"BERR\", \"TOMATO\", \"LEMON\", \"MELON\", \"PEACH\", \"CHERR\", \"PINEAPPLE\", \"CITRUS\"]\n",
    "vegetable_list = [\"PUMPKIN\", \"COLESLAW\", \"SPINACH\", \"BEAN\",\"VEG\", \"POTATO\", \"CARROT\",\"CORN\",\"PEPPER\", \"SQUASH\", \"ONION\", \"MUSHROOM\", \"CUCUMBER\", \"CAULIFLOWER\", \"BROCCOLI\", \"OLIVE\" ]\n",
    "veg_animal_list = [\"DAIRY\",\"MILK\",\"CHEESE\", \"YOGURT\", \"BUTTER\", \"MARGARINE\", \"EGG\", \"HONEY\"]\n",
    "beverage_list = [\"CIDER\", \"BEVERAGE\", \"WATER\", \"COKE\", \"FANTA\", \"SPRITE\",\"LIQUOR\", \"LEMONADE\", \"COCOA\", \"VODKA\", \"BEER\", \"WINE\", \"COFFEE\", \"DRINK\", \"TEA\", \"JUICE\", \"DRNKS\", \"JCE\"]\n",
    "condiment_list = [\"GARLIC\",\"SALSA\",\"FLOUR\", \"DIP\",\"OIL\",\"SEASONING\",\"JELLY\", \"JAM\",\"SAUCE\", \"HERBS\", \"CONDIMENT\", \"TOPPING\", \"SYRUP\", \"DRESSING\", \"KETCHUP\", \"MAYO\", \"DRSNG\"]\n",
    "carbs_list = [\"NOODLES\", \"PASTA\", \"CROUTON\",\"RICE\", \"BREAD\", \"TOAST\", \"CEREAL\", \"OATMEAL\", \"DOUGH\", \"POTATO\"]\n",
    "meal_list = [\"PIZZA\", \"RAMEN\", \"SANDWICH\", \"SALAD\",\"SOUP\",\"ENTREE\", \"DINNER\", \"BREAKFAST\", \"THAI\", \"ASIAN\", \"ITALIAN\", \"MEXICAN\", \"GERMAN\", \"BURRITO\", \"FOOD\", \"ORIENTAL\", \"DISH\", \"KOSHER\"]\n",
    "\n",
    "df_meat, df_rest = filter_dataframe(df_products, meat_list)\n",
    "df_seafood, df_rest = filter_dataframe(df_rest, seafood_list)\n",
    "df_sweets_snack, df_rest = filter_dataframe(df_rest, sweets_snack_list)\n",
    "df_fruits, df_rest = filter_dataframe(df_rest, fruits_list)\n",
    "df_vegetable, df_rest = filter_dataframe(df_rest, vegetable_list)\n",
    "df_veg_animal, df_rest = filter_dataframe(df_rest, veg_animal_list)\n",
    "df_beverage, df_rest = filter_dataframe(df_rest, beverage_list)\n",
    "df_condiments, df_rest = filter_dataframe(df_rest, condiment_list)\n",
    "df_carbs, df_rest = filter_dataframe(df_rest, carbs_list)\n",
    "df_meals, df_rest = filter_dataframe(df_rest, meal_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also group some categories in Vegetarian and Non-vegetarian (note we have left out categories such as Meals or Condiments as one would need to have the precise list of ingredients to know if a spefific product is vegetarian or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veg = df_fruits.append(df_vegetable).append(df_veg_animal).append(df_carbs)\n",
    "df_non_veg = df_meat.append(df_seafood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_food = pd.concat([df_meat, df_seafood, df_sweets_snack, df_fruits, df_vegetable, df_veg_animal, df_beverage, df_condiments, df_carbs, df_meals])\n",
    "df_food.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfoods_no_size = df_food[df_food[\"curr_size_of_product\"] == \" \"].count()[\"product_id\"]\n",
    "nfoods = df_food.count()[\"product_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have sizes informations for {:.2f}% of food products\".format(100 * (1 - nfoods_no_size / nfoods)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with all remaining products (even non-food products)\n",
    "df_left_all = df_products[\n",
    "    ~df_products.index.isin(\n",
    "        np.concatenate(\n",
    "            (\n",
    "                df_meat.index,\n",
    "                df_seafood.index,\n",
    "                df_sweets_snack.index,\n",
    "                df_fruits.index,\n",
    "                df_vegetable.index,\n",
    "                df_beverage.index,\n",
    "                df_veg_animal.index,\n",
    "                df_condiments.index,\n",
    "                df_carbs.index,\n",
    "                df_meals.index,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a list of the categories and of the dataframes to easily access them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categories of food \n",
    "categories = ['Meat', 'Seafood', 'Sweet snacks', 'Fruits', 'Vegetables', 'Veg animal', 'Beverage', 'Condiments', 'Carbs', 'Meals', 'Other']\n",
    "df_list = (df_meat, df_seafood, df_sweets_snack, df_fruits, df_vegetable, \n",
    "           df_veg_animal, df_beverage, df_condiments, df_carbs, df_meals, df_left_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the distribution of products we have in the dataset across these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_count = pd.DataFrame(columns = ['Category', 'Count'])\n",
    "for categ,df in enumerate(df_list):\n",
    "    products_count = products_count.append({'Category':categories[categ],\n",
    "                                            'Count':len(df)}, ignore_index=True)\n",
    "products_count.plot.bar()\n",
    "plt.xticks(np.arange(11), categories, rotation = 'horizontal')\n",
    "plt.ylabel('Number of products')\n",
    "plt.title('Distribution of products in the designed categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campaigns & coupons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important aspects of this dataset is the list of promoting campaigns conducted among the households, with informations on the related coupons. Let's explore the 4 dataframes related to these (2 specifically about the campaigns and 2 specifically about the coupons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_desc = pd.read_csv('dunnhumby/campaign_desc.csv')\n",
    "df_campaign_desc.columns = map(str.lower, df_campaign_desc.columns)\n",
    "df_campaign_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of campaigns: {} '.format(len(df_campaign_desc.campaign)))\n",
    "print('Unique description values: {} '.format(df_campaign_desc.description.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add duration column to dataframe\n",
    "df_campaign_desc['duration'] = df_campaign_desc['end_day'] - df_campaign_desc['start_day'] \n",
    "df_campaign_desc.sort_values(by = 'duration', ascending = False).reset_index(drop=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the longest campaign was on for 161 days, but this seems to be an outlier as the others don't last for more than 60-70 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for camp in ['TypeA', 'TypeB', 'TypeC']:\n",
    "    print('Campaigns of %s range between %d and %d days' %(camp, df_campaign_desc[df_campaign_desc.description == camp].duration.min(),\n",
    "                                                     df_campaign_desc[df_campaign_desc.description == camp].duration.max()))\n",
    "    print('with a mean duration of %.2f days and a median duration of %.2f days. \\n' \n",
    "            %(df_campaign_desc[df_campaign_desc.description == camp].duration.mean(),\n",
    "              df_campaign_desc[df_campaign_desc.description == camp].duration.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the campaign table, containing the campaigns that each household has benefited from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_table = pd.read_csv('dunnhumby/campaign_table.csv')\n",
    "df_campaign_table.columns = map(str.lower, df_campaign_table.columns)\n",
    "df_campaign_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both campaign tables (one containing the data and the other providing further descriptions regarding each campaign) have one \"key\" in common, namely the `campaign id` we can join (full outer join) these two tables in order to have all information in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_full = pd.merge(df_campaign_desc, df_campaign_table, on='campaign', how='outer').rename(columns={\"descritpion_x\": \"description\"}).drop(columns='description_y')\n",
    "# Map the lowering function to all column names\n",
    "df_campaign_full.columns = map(str.lower, df_campaign_full.columns)\n",
    "df_campaign_full.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributional analysis of how campaigns reached each household\n",
    "Let's observe how the campaigns have reached the households. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = df_campaign_table.groupby(by = 'household_key').campaign.count().max()\n",
    "df_campaign_table.groupby(by = 'household_key').campaign.count().hist(bins = nbins)\n",
    "plt.title('Distribution of how many households have benefitted from how many campaigns')\n",
    "plt.xlabel('Number of campaigns')\n",
    "plt.ylabel('Number of households')\n",
    "plt.savefig(\"test.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot we can see, that most households have only benefited from 1 campaign and very few have made use of more than 10 campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_full.groupby(by = 'household_key').campaign.count().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Mean number of campaigns:_ ~4.55\n",
    "\n",
    "_Max number of campaigns:_ 17\n",
    "\n",
    "_Looking at the quantiles:_\n",
    "\n",
    "* 25% of the households have made use of 2 campaigns or less. \n",
    "* 75% of the households have made use of 6 campaigns or less.\n",
    "* 95% of the households have made use of 10 campaigns or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timespan of the campaigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the different campaigns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the datas per campaign type for better visualization\n",
    "beginA = df_campaign_desc[df_campaign_desc.description == 'TypeA']['start_day'].values\n",
    "endA =   df_campaign_desc[df_campaign_desc.description == 'TypeA']['end_day'].values\n",
    "beginB = df_campaign_desc[df_campaign_desc.description == 'TypeB']['start_day'].values\n",
    "endB =   df_campaign_desc[df_campaign_desc.description == 'TypeB']['end_day'].values\n",
    "beginC = df_campaign_desc[df_campaign_desc.description == 'TypeC']['start_day'].values\n",
    "endC =   df_campaign_desc[df_campaign_desc.description == 'TypeC']['end_day'].values\n",
    "\n",
    "#Plot the timespan of each campaign (note the +1 to match the indexes starting at 0 with the campaign numbers starting at 1)\n",
    "plt.barh(df_campaign_desc[df_campaign_desc.description == 'TypeA'].index.values + 1,  endA-beginA, left=beginA, \n",
    "         color = 'red', label = 'TypeA')\n",
    "plt.barh(df_campaign_desc[df_campaign_desc.description == 'TypeB'].index.values + 1,  endB-beginB, left=beginB, \n",
    "         color = 'blue', label = 'TypeB')\n",
    "plt.barh(df_campaign_desc[df_campaign_desc.description == 'TypeC'].index.values + 1,  endC-beginC, left=beginC, \n",
    "         color = 'green', label = 'TypeC')\n",
    "\n",
    "#Add title & legend\n",
    "plt.title('Timespan of each campaign')\n",
    "plt.ylabel('Campaign')\n",
    "plt.yticks(df_campaign_desc.index + 1)\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coupons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading and having a look at the two dataframes related to coupons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coupon_redempt = pd.read_csv('dunnhumby/coupon_redempt.csv')\n",
    "df_coupon_redempt.columns = map(str.lower, df_coupon_redempt.columns)\n",
    "df_coupon_redempt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coupon = pd.read_csv('dunnhumby/coupon.csv')\n",
    "df_coupon.columns = map(str.lower, df_coupon.columns)\n",
    "df_coupon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study how many coupons were redeemed for each campaign:\n",
    "we start by adding a column stating if a coupon has been redeemed or not. `coupon_upc` here is the unique identifier of each coupon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coupon['redeemed'] = df_coupon['coupon_upc'].isin(df_coupon_redempt['coupon_upc'])\n",
    "df_coupon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe how many coupons were distributed and redeemed by households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_redempt_coupons = df_coupon.redeemed[df_coupon.redeemed == True].shape[0]\n",
    "redemption_rate = num_redempt_coupons / df_coupon.shape[0] * 100\n",
    "print(\"{} of {} coupons in total were actually redeemed, which corresponds to a redemption rate of {} %.\".format(num_redempt_coupons,df_coupon.shape[0], round(redemption_rate,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above rate is suprisingly high...\n",
    "\n",
    "So let us have a look how many coupons were distributed within the course of each campaign, by combining the informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount of coupons per campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by combining the campaigns data with the coupons data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_desc.sort_values(by = 'campaign', inplace = True)\n",
    "df_campaign_desc.set_index(keys = 'campaign', drop = True, inplace = True)\n",
    "df_campaign_desc['distributed'] = df_coupon.groupby(by = 'campaign').redeemed.count() #Number of coupons distributed\n",
    "df_campaign_desc['redeemed'] = df_coupon[~df_coupon['redeemed']].groupby(by = 'campaign').redeemed.count() #Number of coupons redeemed\n",
    "df_campaign_desc.fillna(0, inplace = True) \n",
    "df_campaign_desc['beneficiary'] = df_campaign_table.groupby(by = 'campaign').description.count() #Number of beneficiary households\n",
    "df_campaign_desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe these results in a bar plot. For each campaign we plot the amount of coupon distributed and redeemed, and we add the type of the campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set width of bar\n",
    "barWidth = 0.4\n",
    " \n",
    "# set height of bar\n",
    "coupon_distributed = df_campaign_desc['distributed']\n",
    "coupon_redeemed = df_campaign_desc['redeemed']\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r = np.arange(len(coupon_distributed)+1)\n",
    "r1 = [x - barWidth/2 for x in r[1:]]\n",
    "r2 = [x + barWidth/2 for x in r[1:]]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(r1, coupon_distributed, width=barWidth, edgecolor='white', label='Distributed coupons')\n",
    "plt.bar(r2, coupon_redeemed, width=barWidth, edgecolor='white', label='Redeemed coupons')\n",
    "plt.yscale('log') #Better visualizations because very different amounts of coupon between campaign\n",
    " \n",
    "# Attach a text label above each bar in *bars*, displaying the campaign type\n",
    "type_c = df_campaign_desc.description.values\n",
    "for i in range(len(coupon_distributed.values)):    \n",
    "    plt.annotate('{}'.format(type_c[i]),\n",
    "                xy=(r[i+1], coupon_distributed.values[i]),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom',\n",
    "                fontweight = 'bold',\n",
    "                fontsize = 8)\n",
    "  \n",
    "    \n",
    "# Add title and legend\n",
    "plt.title('Amount of coupons per campaign')\n",
    "plt.xlabel('Campaign')\n",
    "plt.xticks(df_campaign_desc.index)\n",
    "plt.ylabel('Amount of coupons')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the y-axis is in logarithmic scale: this is done because the amounts of coupons differ a lot from one campaign to another. Moreover, we can see the difference between the amount of coupon distributed and redeemed differs a lot from one campaign to another. It goes from cases where it is nearly null (such as campaign 1) to extreme cases where no coupons were redeemed (such as in campaigns 6, 15 and 24). To understand what happened here, we will need to inspect more precisely each campaign and join our results with data about the products and the transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advertisement analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, informations about the displayement of the products in the stores and of the ads in the mailer are prodided. Let's have a look at these, grouping the products by categories of food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_causal = pd.read_csv('dunnhumby/causal_data.csv')\n",
    "df_causal.columns = map(str.lower, df_causal.columns)\n",
    "df_causal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by changing the 'display' and 'mailer' values to make them more explicit. We will define two dict and then use the *map()* method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dict = {'0':'Not on Display', '1':'Store Front', '2':'Store Rear', '3':'Front End Cap',\n",
    "                '4':'Mid-Aisle End Cap', '5':'Read End Cap', '6':'Side-Aisle End Cap', '7':'In-Aisle' ,\n",
    "                '9':'Secondary Location Display' ,'A':'In-Shelf' }\n",
    "mailer_dict = {'0':'Not on ad', 'A':'Interior page feature', 'C':'Interior page line item', \n",
    "              'D':'Front page feature', 'F':'Back page feature', 'H':'Wrap front feature',\n",
    "              'J':'Wrap interior coupon', 'L':'Wrap back feature', 'P':'Interior page coupon',\n",
    "              'X':'Free on interior page', 'Z':'Free on front page, back page or wrap'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_causal.display = df_causal.display.apply(lambda x: str(x))\n",
    "df_causal.display = df_causal.display.map(display_dict, na_action = 'ignore')\n",
    "df_causal.mailer = df_causal.mailer.map(mailer_dict, na_action = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe how many products we have in each category, both for display and mailer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_causal.groupby(by = 'display').count().mailer.plot(kind = 'barh')\n",
    "plt.xlabel('Number of products')\n",
    "plt.ylabel('')\n",
    "plt.title('Number of products for each type of displacement in a store')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, many products are simply not in display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_causal.groupby(by = 'mailer').count().display.plot(kind = 'barh')\n",
    "plt.xlabel('Number of products')\n",
    "plt.ylabel('')\n",
    "plt.title('Number of products for each type of displacement in the mailer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the largest numbers of ads are in the interior or front pages (or simply not on ad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now analyse the ads distribution for each category of food previously defined. To do this, we will define a function ads_categories() that will take a dataframe (one of the 11 dataframes of the categories of food) and return the distribution of ads (as percentages). A show_plot parameter will decide if we want to directly plot the distribution in a bar plot. We return both the raw count and the proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ads_categories(df, categ = '', show_plot = False):\n",
    "    count = df.merge(df_causal, on = 'product_id').groupby(by = 'mailer').display.count()\n",
    "    proportions = count.map(lambda x: 100*x/count.sum())\n",
    "    \n",
    "    if show_plot:\n",
    "        proportions.plot.bar()\n",
    "        plt.title('Proportion of mailer ads for %s' %categ)\n",
    "        plt.xlabel('Type of ads')\n",
    "        plt.xticks(rotation = 30)\n",
    "        plt.ylabel('Proportion of ads [%]')\n",
    "        plt.show()\n",
    "    \n",
    "    return (proportions,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give an example for meat for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prop_meat,count_meat) = ads_categories(df_meat, 'meat', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataframe to store the ads proportions for each category of food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads_prop = pd.DataFrame(index = mailer_dict.values(), columns = categories)\n",
    "df_ads_prop.sort_index(inplace = True)\n",
    "df_ads_prop.index.name = 'Type of ads'\n",
    "\n",
    "df_ads_count = pd.DataFrame(index = mailer_dict.values(), columns = categories)\n",
    "df_ads_count.sort_index(inplace = True)\n",
    "df_ads_count.index.name = 'Type of ads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(df_list):\n",
    "    df_ads_prop[categories[i]] = ads_categories(df)[0]\n",
    "    df_ads_count[categories[i]] = ads_categories(df)[1]\n",
    "df_ads_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice some values are set to NaN. This happens because, if there are no ads of the specific type of mailer, it is simply missing from the series. We can therefore simply fill all NaN values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads_prop.fillna(0, inplace = True)\n",
    "df_ads_count.fillna(0, inplace = True)\n",
    "df_ads_prop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads_prop.transpose().plot.bar(stacked = True) #The transpose() is needed to get the right kind of stacked barplot\n",
    "plt.title('Proportions of types of ads per category of food')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Proportions of types of ads [%]')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.28,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look more precisely at Not on ad, Front page feature, Interior page feature and Back page feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = range(len(categories)), y = df_ads_prop.transpose()['Not on ad'], \n",
    "            label = 'Not on ad', marker = 'x', s = 200)\n",
    "plt.scatter(x = range(len(categories)), y = df_ads_prop.transpose()['Front page feature'], \n",
    "            label = 'Front page feature', marker = '+', s = 200)\n",
    "plt.scatter(x = range(len(categories)), y = df_ads_prop.transpose()['Interior page feature'], \n",
    "            label = 'Interior page feature', marker = 'd', s = 200)\n",
    "plt.scatter(x = range(len(categories)), y = df_ads_prop.transpose()['Back page feature'], \n",
    "            label = 'Back page feature', marker = '^', s = 200)\n",
    "plt.xticks(range(len(categories)), list(categories))\n",
    "plt.title('Proportions of main types of ads per category of food')\n",
    "plt.xlabel('Category of food')\n",
    "plt.ylabel('Percentage of ads [%]')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.2,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that meat ads are mainly situated in the interior pages with very few in the back pages. On the contrary, fruits have much more ads in the back pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some households, we have additional informations (such as the age, the income and the marital status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_dict = {\"A\": \"Married\", \"B\": \"Single\", \"U\": \"Unknown\"}\n",
    "household_size_dict = {\"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5+\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = pd.read_csv(\"dunnhumby/hh_demographic.csv\")\n",
    "df_demo.columns = map(str.lower, df_demo.columns)\n",
    "\n",
    "# Replace cryptic character with actual label\n",
    "df_demo[\"married\"] = df_demo[\"marital_status_code\"].apply(\n",
    "    lambda code: marital_dict[code]\n",
    ")\n",
    "\n",
    "# Create new column with numerical household size\n",
    "for hh_str, hh_int in household_size_dict.items():\n",
    "    df_demo[\"household_size_desc_numeric\"] = df_demo[\"household_size_desc\"].replace(\n",
    "        hh_str, hh_int\n",
    "    )\n",
    "df_demo[\"household_size_desc_numeric\"] = df_demo[\"household_size_desc_numeric\"].astype(int)\n",
    "    \n",
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see for how many households we have these informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have demographics informations for %d households.' %len(df_demo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis we will focus on people grouped according to three criteria: marital status, income and age. Let's observe the distributions of those groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_index = [\n",
    "    \"Under 15K\",\n",
    "    \"15-24K\",\n",
    "    \"25-34K\",\n",
    "    \"35-49K\",\n",
    "    \"50-74K\",\n",
    "    \"75-99K\",\n",
    "    \"100-124K\",\n",
    "    \"125-149K\",\n",
    "    \"150-174K\",\n",
    "    \"175-199K\",\n",
    "    \"200-249K\",\n",
    "    \"250K+\",\n",
    "]\n",
    "\n",
    "df_demo[\"income_desc\"].value_counts().sort_index().reindex(salary_index).plot(kind=\"bar\")\n",
    "plt.xlabel('Income')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Number of households')\n",
    "plt.title('Income distribution of the households')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[\"age_desc\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.xlabel('Age')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Number of households')\n",
    "plt.title('Age distribution of the households')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marital status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[\"married\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.xlabel('Marital status')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Number of households')\n",
    "plt.title('Marital status distribution of the households')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a lot of Unknown in the dataset, we will try to define new groups about the 'type' of family, using the *hh_comp_desc* informations. We define 4 categories: Adults No Kids, With kids (meaning one or more adults, with kids), Single and Unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_comp = {'2 Adults No Kids':'Adults No Kids', \n",
    "           '1 Adult Kids':'With kids',\n",
    "           '2 Adults Kids':'With kids', \n",
    "           'Single Female':'Single',\n",
    "           'Single Male':'Single',\n",
    "           'Unknown':'Unknown'}\n",
    "df_demo.hh_comp_desc = df_demo.hh_comp_desc.map(hh_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the distribution of households across these new groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.hh_comp_desc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 3 main groups of approximately the same size, and only 73 unknown compositions. Let's see if we can infere the composition of these remaining 73 households from other informations on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[df_demo.hh_comp_desc == 'Unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the marital status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[df_demo.hh_comp_desc == 'Unknown'].married.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the Single people. Do they have any kids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[df_demo.hh_comp_desc == 'Unknown'][df_demo.married == 'Single']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two households with marital status as Single have no kids, so they can enter in the category Single for the household composition criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.at[663, 'hh_comp_desc'] = 'Single'\n",
    "df_demo.at[758, 'hh_comp_desc'] = 'Single'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at the 9 households marked as married."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[df_demo.hh_comp_desc == 'Unknown'][df_demo.married == 'Married']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of them have kids and other don't. The first will be set to the group 'With kids' and the second to the group 'Adults No Kids'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_demo[df_demo.hh_comp_desc == 'Unknown'][df_demo.married == 'Married'].index.values:\n",
    "    if df_demo.at[i, 'kid_category_desc'] == 'None/Unknown':\n",
    "        df_demo.at[i, 'hh_comp_desc'] = 'Adults No Kids'\n",
    "    else:\n",
    "        df_demo.at[i, 'hh_comp_desc'] = 'With kids'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can have a look at the *household_size_desc_numeric* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[df_demo.hh_comp_desc == 'Unknown'].household_size_desc_numeric.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the households with more than 1 person. We will set them to either 'With kids' or 'Adults No Kids'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[df_demo.hh_comp_desc == 'Unknown'][df_demo.household_size_desc_numeric == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[df_demo.hh_comp_desc == 'Unknown'][df_demo.household_size_desc_numeric == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.at[375, 'hh_comp_desc'] = 'With kids'\n",
    "df_demo.at[579, 'hh_comp_desc'] = 'With kids'\n",
    "df_demo.at[628, 'hh_comp_desc'] = 'With kids'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the *household_size_desc_numeric* column of the remaining Unknown households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[df_demo.hh_comp_desc == 'Unknown'].household_size_desc_numeric.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all are households composed by just one person, we can therefore assign them to the Single category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_demo[df_demo.hh_comp_desc == 'Unknown'].index.values:\n",
    "    df_demo.at[i, 'hh_comp_desc'] = 'Single'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have assigned each household to a meaningful category. Let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.hh_comp_desc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo[\"hh_comp_desc\"].value_counts().plot(kind=\"bar\")\n",
    "plt.xlabel('Household composition')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Number of households')\n",
    "plt.title('Household composition distribution of the households')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aspect of the dataset is the list of all transcations the households have done. In this section, we will focus ourselfs on the prices of the products. We will start by some basic statistical analysis and then group products in the previously defined categories and try to extract some meaningful conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction = pd.read_csv('dunnhumby/transaction_data.csv')\n",
    "df_transaction.columns = map(str.lower, df_transaction.columns)\n",
    "df_transaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, how many households do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_households = len(df_transaction[\"household_key\"].unique())\n",
    "print('The dataset contains {} individual households.'.format(number_households))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clarify what the less obvious columns represent. The *sales_value* is the amount of dollar received by the retailer for a particular sale. The *retail_disc* is a discount applied due to retailer's loyalty card program. The *coupon_disc* is a discount applied due to manufacturer coupon and *coupon_match_disc* a discount applied due to retailer's match of manufacturer coupon. Finally, the *trans_time* corresponds to the time of the day the purchase was made (ranging from 0 to 2359, i.e. from 0:00 to 23:59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have %d transactions in our dataset.' %len(df_transaction))\n",
    "print('We have %d transactions in our dataset with QUANTITY = 0.' %len(df_transaction[df_transaction.quantity == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by discarding all transcations for which the quantity is null (as no item has actually been sold in these cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction = df_transaction[df_transaction.quantity > 0]\n",
    "print('We have %d transactions left in our dataset.' %len(df_transaction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the actual product prices (with or without loyalty card). To do so, we use the following formulas (as explained in the user guide):\n",
    "- Loyalty card price = (sales_value – (retail_disc + coupon_match_disc))/quantity\n",
    "- Non-loyalty card price = (sales_value – coupon_match_disc)/quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction['loyalty_card_price'] = (df_transaction['sales_value'] - (df_transaction['retail_disc'] + df_transaction['coupon_match_disc']))/df_transaction['quantity']\n",
    "df_transaction['no_loyalty_card_price'] = (df_transaction['sales_value'] - df_transaction['coupon_match_disc'])/df_transaction['quantity']\n",
    "df_transaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(df_transaction['sales_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('50% of the transations are below ' + str(df_transaction['sales_value'].median()) + '$.')\n",
    "print('80% of the transations are below ' + str(df_transaction['sales_value'].quantile(0.8)) + '$.')\n",
    "print('There are %d transactions with sales value 0.' %len(df_transaction[df_transaction['sales_value'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual price (with and without loyalty card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(df_transaction['loyalty_card_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('50% of the prices (with loyalty card) are below ' + str(df_transaction['loyalty_card_price'].median()) + '$.')\n",
    "print('80% of the prices (with loyalty card) are below ' + str(df_transaction['loyalty_card_price'].quantile(0.8)) + '$.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(df_transaction['no_loyalty_card_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('50% of the prices (without loyalty card) are below ' + str(df_transaction['no_loyalty_card_price'].median()) + '$.')\n",
    "print('80% of the prices (without loyalty card) are below ' + str(df_transaction['no_loyalty_card_price'].quantile(0.8)) + '$.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounts (retailer and coupons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(df_transaction['retail_disc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(df_transaction['coupon_disc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(df_transaction['coupon_match_disc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The TRANS_TIME ranges between %d and %d.' %(df_transaction['trans_time'].min(), df_transaction['trans_time'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories of food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us join the products and transactions data for more insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_food_trans = pd.merge(df_food, df_transaction, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what percentile of the households did not buy meat and seafood products at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meat_seafood = df_meat.append(df_seafood)\n",
    "df_meat_seafood_trans = pd.merge(df_meat_seafood, df_transaction, on='product_id', how='left')\n",
    "number_household_meat_seafood = len(df_meat_seafood_trans[\"household_key\"].unique())\n",
    "number_hh_no_meat_seafood = number_households-number_household_meat_seafood\n",
    "prop_hh_no_meat_seafood = number_hh_no_meat_seafood/number_households\n",
    "print(\"Number of households who did not purchased any meat or seafood: %d (%0.2f%% of the total)\" %(number_hh_no_meat_seafood, prop_hh_no_meat_seafood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the meat and seafood weekly transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_meat_sea = df_meat_seafood_trans.groupby(\"week_no\").count()[\"product_id\"]\n",
    "weekly_trans = df_food_trans.groupby(\"week_no\").count()[\"product_id\"]\n",
    "plt.plot(weekly_meat_sea)\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Number of transactions\")\n",
    "plt.title(\"Meat and Seafood Transactions [Weekly]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall there is no upwards or downwards trend of meat and seafood transactions. But it is weird that the meat and seafood transactions rose rapidly in the first few weeks. It is likely that in the beginning not all transactions were caught. We take a look at the relation between the number of transactions and the number of meat and seafood transactions. If the rise in the beginning is only due to the fact that not all transactions were recorded, then the relation should be linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(weekly_trans, weekly_meat_sea)\n",
    "plt.xlabel(\"Total number of Transactions\")\n",
    "plt.ylabel(\"Number of Meat and Seafood Transactions\")\n",
    "plt.title(\"Meat and Seafood Transaction in Relation to overall Number of Transactions [Weekly]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation is indeed linear.\n",
    "\n",
    "Next we take a look at the number of households which weekly buy meat or seafood products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_meat_seafood_trans.groupby(\"week_no\")[\"household_key\"].nunique())\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Number of Households\")\n",
    "plt.title(\"Households that purchased Meat and Seafood Transactions [Weekly]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number again stays relatively stable at around 800-900. That is surprising, because the number of households which did purchase a meat product in the two years is 2475.\n",
    "\n",
    "Let's check the number of households, which go vegetarian in a week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_vege = df_food_trans.groupby(\"week_no\")[\"household_key\"].nunique()-df_meat_seafood_trans.groupby(\"week_no\")[\"household_key\"].nunique()\n",
    "plt.plot(weekly_vege)\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Number of Households\")\n",
    "plt.title(\"Relation of overall households and households that bought meat or seafood [Weekly]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly the number is super high considering that almost all households do not live completely vegetarian. Lets check the frequency of the meat and seafood purchases of all households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_purchase = df_meat_seafood_trans.groupby(\"household_key\").count()[\"product_id\"]\n",
    "#print(house_purchase.sort_values(ascending=False).head(10))\n",
    "plt.hist(house_purchase, bins=50)\n",
    "plt.xlabel(\"Number of meat and seafood transactions\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histrogram for Meat and Seafood transaction in a household\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:30px 30px; background-color: #3388FF; text-align: center; font-size:30px;\">\n",
    "   <strong>Demographics and Food</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to find out if there are **different consumptions behaviours** between **different groups of people** (based on their age, their income and the household composition). To do so, we will combine the informations from the demographic dataframe and the transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the transactions with the demographics informations\n",
    "df_transaction_per_household = df_transaction.merge(df_demo, on=\"household_key\")\n",
    "\n",
    "#Assign a category to each transaction\n",
    "df_transaction_per_household[\"category\"] = \"Unknown\"\n",
    "#Let's mark everything with the corresponding category\n",
    "for categ,df in enumerate(df_list):\n",
    "    df_transaction_per_household.loc[\n",
    "        df_transaction_per_household[\"product_id\"].isin(df[\"product_id\"]), \"category\"\n",
    "    ] = categories[categ]\n",
    "\n",
    "#Check: we should only have products of the 11 categories previously designed\n",
    "df_transaction_per_household.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the sizes of the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_per_household = df_transaction_per_household.merge(df_products[[\"product_id\",\"curr_size_of_product\", \"sub_commodity_desc\"]], \n",
    "                                                                  on=\"product_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a dataframe with the ratio of transactions in each category for each household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataframe\n",
    "ratio_df = (\n",
    "    df_transaction_per_household.groupby(\"household_key\")[\"category\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .unstack()\n",
    ")\n",
    "#If a household has no transactions in a given category, the count will be set to NaN, we simple fill with 0\n",
    "ratio_df.fillna(0, inplace = True)\n",
    "\n",
    "# Drop the multi-columns, so we can use simple indexing for ratio_meat \n",
    "ratio_df.columns = ratio_df.columns.droplevel()\n",
    "\n",
    "#Compute the ratios for each category of food\n",
    "ratio_df['Total'] = 0\n",
    "for categ in categories:\n",
    "    ratio_df['Total'] += ratio_df[categ]\n",
    "for categ in categories:\n",
    "    #name = 'ratio ' + categ.lower()\n",
    "    ratio_df[categ] = ratio_df[categ] / ratio_df[\"Total\"]\n",
    "\n",
    "# Get household_key as a column\n",
    "ratio_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meat consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on meat consumption first. We create a dataframe *df_meat_per_household* with only food, dropping all transactions with no sizes of the product bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meat_per_household = df_transaction_per_household[df_transaction_per_household[\"category\"] == \"Meat\"]\n",
    "df_meat_per_household = df_meat_per_household[df_meat_per_household.curr_size_of_product != \" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We design some utility functions to treat the sizes of product (since there are many different types). To do so, we use the libraries *measurement* and *fractions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from measurement import utils\n",
    "from fractions import Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "\n",
    "simple_oz_rex = re.compile(r\".*?(\\d*\\.?\\d*)\\s*(oz|ounce)\")\n",
    "simple_lb_rex = re.compile(r\".*?(\\d*\\.?\\d*)\\s*lb\")\n",
    "oz_fractional_rex = re.compile(r\"(\\d+)\\s+(\\d+\\/\\d+)\\s*oz\")\n",
    "\n",
    "#Parse the sizes in a more manageable format\n",
    "def parse_size(size):\n",
    "    size = size.lower()\n",
    "    \n",
    "    match = oz_fractional_rex.match(size)\n",
    "    if match:\n",
    "        whole = match.group(1)\n",
    "        fraction = float(Fraction(match.group(2)))\n",
    "        \n",
    "        return (int(whole) + fraction, \"oz\")\n",
    "    \n",
    "    match = simple_oz_rex.match(size)\n",
    "    if match:\n",
    "        return (match.group(1), \"oz\")\n",
    "    \n",
    "    match = simple_lb_rex.match(size)\n",
    "    if match:\n",
    "        return (match.group(1), \"lb\")\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "#Transform american units into kg\n",
    "def fix_size(size):\n",
    "    clean_size = parse_size(size)\n",
    "    \n",
    "    if not isinstance(clean_size, tuple) and math.isnan(clean_size):\n",
    "        return np.nan\n",
    "    \n",
    "    if isinstance(clean_size, float):\n",
    "        print(\"---_>>>>>\", clean_size)\n",
    "        \n",
    "    return utils.guess(*clean_size).kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply these functions to our meat dataframe to obtain the total weight (in kg) of meat bought by each household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the equivalent weight of each meat product bought\n",
    "df_meat_per_household[\"weight(kg)\"] = df_meat_per_household.curr_size_of_product.apply(fix_size)\n",
    "#Group by household_key to get the total amount of meat (in kg) bought by each household\n",
    "weight_meat_per_household = df_meat_per_household.groupby(\"household_key\")[[\"weight(kg)\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the duration, i.e. the number of days between the first and the last transaction for meat, for each household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_meat_per_household[\"duration(days)\"] = (\n",
    "    df_meat_per_household.groupby(\"household_key\")[\"day\"].max()\n",
    "    - df_meat_per_household.groupby(\"household_key\")[\"day\"].min()\n",
    "    + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we don't have any null durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_null_duration = len(weight_meat_per_household[\"duration(days)\"][weight_meat_per_household[\"duration(days)\"] == 0].index.values)\n",
    "print('We have %d households with a null duration in our database.' %number_null_duration)\n",
    "#weight_meat_per_household.drop(index=invalid_duration_hhkey, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will transform the meat ratio in the weight of meat bought per day by each household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine our newly computed ratio_meat and the old household data\n",
    "df_demo_w_meat = df_demo.merge(ratio_df[[\"household_key\", \"Meat\"]], on=\"household_key\")\n",
    "\n",
    "# Normalize our ratio_meat by the size of the households\n",
    "df_demo_w_meat[\"ratio meat\"] = df_demo_w_meat[\"Meat\"]/df_demo_w_meat[\"household_size_desc_numeric\"]\n",
    "\n",
    "# Combine it with the weight\n",
    "df_demo_w_meat = df_demo_w_meat.merge(weight_meat_per_household, on=\"household_key\")\n",
    "\n",
    "# Normalize the same way we did \n",
    "df_demo_w_meat[\"weight(kg)\"] /= df_demo_w_meat[\"household_size_desc_numeric\"]\n",
    "df_demo_w_meat[\"weight(kg/day)\"] = df_demo_w_meat[\"weight(kg)\"] / df_demo_w_meat[\"duration(days)\"]\n",
    "\n",
    "df_demo_w_meat[['household_key', 'age_desc', 'hh_comp_desc', 'income_desc', 'ratio meat', 'weight(kg/day)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get a sense of the data, we'll plot it (grouped by three different attributes: household composition, age and income). To do so, we define a function *plot_categ_groups()* that takes the dataframe from which we want to plot our data, the category we are interested in and the criteria to split the households into different groups. This function is general and can be applied to any dataframe, as long as the format is correct (each household has a tag to assign it to a group and the quantity to plot for the category of food we are interested in). Note the *y_label* also has to be specified in order to make this function as general as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categ_groups(df, categ, group, y_label, title):\n",
    "    #Get the data to plot\n",
    "    data = df.groupby(group).mean()[categ]\n",
    "    #Compute the errors on the means with bootstrap resampling\n",
    "    means = []\n",
    "    for _ in range(500):\n",
    "        bs_resample_per_group = df.groupby(group).apply(\n",
    "            lambda x: x.sample(frac=1.0, replace=True)\n",
    "        )\n",
    "\n",
    "        bs_mean = bs_resample_per_group[categ].groupby(group).mean()\n",
    "\n",
    "        means.append(bs_mean)\n",
    "    means = pd.DataFrame(means)\n",
    "\n",
    "    qupper = means.quantile(0.975)\n",
    "    qlower = means.quantile(0.025)\n",
    "    \n",
    "    #Order the income indexes\n",
    "    if group == \"income_desc\":\n",
    "        data = data.reindex(salary_index)\n",
    "        \n",
    "    #Plot the data\n",
    "    dict_group = {'hh_comp_desc':'Household composition',\n",
    "                  'age_desc':'Age',\n",
    "                  'income_desc':'Income'}    \n",
    "    data.plot.bar(yerr=[data - qlower, qupper - data], capsize=10)\n",
    "    plt.xticks(rotation = 'horizontal')\n",
    "    plt.xlabel(dict_group[group])\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categ_groups(df_demo_w_meat, 'weight(kg/day)', 'hh_comp_desc', \n",
    "                  'Percentage of meat of overall shoppings', \n",
    "                  'Percentages of meat purchases (95% CI) given the household composition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categ_groups(df_demo_w_meat, 'weight(kg/day)', 'income_desc', \n",
    "                  'Percentage of meat of overall shoppings', \n",
    "                  'Percentages of meat purchases (95% CI) given the income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categ_groups(df_demo_w_meat, 'weight(kg/day)', 'age_desc', \n",
    "                  'Percentage of meat of overall shoppings', \n",
    "                  'Percentages of meat purchases (95% CI) given the age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some interesting things:\n",
    "\n",
    "* Single people buy a lot more meat than households with multiple people\n",
    "* As the income rises, the amount of meat bought decreases. It then sharply increases for incomes of over 250k\n",
    "* There do not seem to be any notable differences among groups of different ages\n",
    "\n",
    "To confirm our results, we will run some statistical tests (ANOVA or its non-parametric alternative, the Kruskal-Wallis test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA test\n",
    "The following section is based on the explanations of https://pythonfordatascience.org/anova-python/ by Corey Bryant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANOVA test can be thought as an extension of the t-test. The goal is to compare the means of a condition among more than 2 groups. Note that ANOVA is an omnibus test: it tests the data as a whole, meaning it won't specifically say where the differences betwen groups are but only if there are some differences. The hypothesis being tested are:\n",
    "* the means of all the groups are the same\n",
    "* the means of some groups are not the same\n",
    "\n",
    "Mathematically speaking, ANOVA is a generalized linear model with the following general regression equation: \n",
    "\\begin{equation}\n",
    "outcome_i = (model) + error_i\n",
    "\\end{equation}\n",
    "where (model) is of the form:\n",
    "\\begin{equation}\n",
    "model = \\Sigma_{i = 1}^{N-1} b_i group_i + b_0\n",
    "\\end{equation}\n",
    "with $b_0$ the intercept and N the number of groups. $error_i$ is the error of the model. Notice the sum only goes up to N-1 as one group must be taken as the intercept.\n",
    "\n",
    "The assumptions that need to be met are:\n",
    "* normality\n",
    "* homoscedasticity (ie homogeneity of variances)\n",
    "* independent observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function provided by the author of the article, Corey Bryant, extends the ANOVA table with the mean squares and the effect sizes (the eta-squared, or R2, and the omega-square, or extended R2). Eta-squared and omega-squared share the same suggested ranges for low (0.01 – 0.059), medium (0.06 – 0.139), and large (0.14+) effect size classification (see https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_table(aov):\n",
    "    aov['mean_sq'] = aov[:]['sum_sq']/aov[:]['df']\n",
    "    \n",
    "    aov['eta_sq'] = aov[:-1]['sum_sq']/sum(aov['sum_sq'])\n",
    "    \n",
    "    aov['omega_sq'] = (aov[:-1]['sum_sq']-(aov[:-1]['df']*aov['mean_sq'][-1]))/(sum(aov['sum_sq'])+aov['mean_sq'][-1])\n",
    "    \n",
    "    cols = ['sum_sq', 'df', 'mean_sq', 'F', 'PR(>F)', 'eta_sq', 'omega_sq']\n",
    "    aov = aov[cols]\n",
    "    return aov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA test for the meat consumption depending on the household composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a dataframe with just the household composition (our categorial independent variable) and the weight(kg/day) (our continuous dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hh_comp = df_demo_w_meat[['hh_comp_desc', 'weight(kg/day)']]\n",
    "#Need to change the name because ols does not recognize the parenthesis\n",
    "data_hh_comp = data_hh_comp.rename(columns = {'weight(kg/day)':'meat_weight'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function summary_cont() in the researchpy library provides some useful descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(data_hh_comp.groupby('hh_comp_desc')['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the model for the ANOVA test and fit it. Remember one of the group will be treated as the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hh_comp = ols('meat_weight ~ C(hh_comp_desc)', data=data_hh_comp)\n",
    "results_hh_comp = model_hh_comp.fit()\n",
    "results_hh_comp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the results of the test (F[2.797]=76.01, p-value=6.12e-31) indicate that the null hypothesis can be rejected: there are some significant differences between the groups tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov_table_hh_comp = sm.stats.anova_lm(results_hh_comp, typ=2)\n",
    "anova_table(aov_table_hh_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both the eta and omega squares have a large effect size (more than 0.14)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check assumptions\n",
    "We need to check the normality and the homoscedasticity of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk's test tests the null hypothesis that the residuals are normally distributed. The results of this test (W=0.83, p-value=1.79e-28) indicates the null hypothesis has to be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.shapiro(results_hh_comp.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levene test tests the null hypothesis that the samples have homegeneous variances. The results of this test (W=41.9, p-value=4.79e-18) indicates the null hypothesis has to be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(data_hh_comp[data_hh_comp.hh_comp_desc == 'Adults No Kids']['meat_weight'], \n",
    "             data_hh_comp[data_hh_comp.hh_comp_desc == 'With kids']['meat_weight'],\n",
    "             data_hh_comp[data_hh_comp.hh_comp_desc == 'Single']['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the assumptions are not met, we use the Kruskal-Wallis test (non-parametric alternative of ANOVA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kruskal(data_hh_comp[data_hh_comp.hh_comp_desc == 'Adults No Kids']['meat_weight'], \n",
    "              data_hh_comp[data_hh_comp.hh_comp_desc == 'With kids']['meat_weight'],\n",
    "              data_hh_comp[data_hh_comp.hh_comp_desc == 'Single']['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the test (H=133.8, p-value=9.02e-30) show that the null hypothesis can be rejected, meaning there are significant differences between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run some post-hoc tests to find where the significant differences are. To do so, we use the Tukey's HSD method that controls for familywise error rate (different method than the classical Bonferroni). The group1 and group2 columns state which groups are being compared, the meandiff column gives the mean difference of the two groups (computed as group2_mean - group1_mean). The p-adj column gives the p-value (note the familywise error is kept at FWER=0.05). The lower and upper colums give the 95% confidence interval for the mean differences and the reject column states if the null hypothesis must be rejected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_hh_comp = MultiComparison(data_hh_comp['meat_weight'], data_hh_comp['hh_comp_desc'])\n",
    "mc_results_hh_comp = mc_hh_comp.tukeyhsd()\n",
    "print(mc_results_hh_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a significant difference between all the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : Since the assumptions were not met, a Kruskall-Wallis test is conducted. The results of the test (H=133.8, p-value=9.02e-30) show that the null hypothesis can be rejected, meaning there are significant differences between the groups. The post-hoc Tukey's HSD method is used to find where the differences are. The results state that: \n",
    "   <ul>\n",
    "       <li>single people buy more meat than people with no kids (difference of means: 0.0388, p-value=0.001)</li>\n",
    "       <li>single people buy more meat than people with kids (difference of means: 0.0511, p-value=0.001)</li>\n",
    "       <li>people with no kids buy more meat than the adults with kids (difference of means: 0.0122, p-value=0.0202)</li>\n",
    "   </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA test for the meat consumption depending on income "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a dataframe with just the income description (our categorial independent variable) and the weight(kg/day) (our continuous dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_income = df_demo_w_meat[['income_desc', 'weight(kg/day)']]\n",
    "data_income = data_income.rename(columns = {'weight(kg/day)':'meat_weight'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe some descriptive statistics on our groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(data_income.groupby('income_desc')['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the model and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_income = ols('meat_weight ~ C(income_desc)', data=data_income)\n",
    "results_income = model_income.fit()\n",
    "results_income.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the results of the test (F[11.788]=3.356, p-value=0.000152) indicate that the null hypothesis can be rejected: there are some significant differences between the groups tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov_table_income = sm.stats.anova_lm(results_income, typ=2)\n",
    "anova_table(aov_table_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both the eta and omega squares have a low effect size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check assumptions\n",
    "We need to check the normality and the homoscedasticity of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk's test tests the null hypothesis that the residuals are normally distributed. The results of this test (W=0.80, p-value=3.72e-30) indicates the null hypothesis has to be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.shapiro(results_income.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levene test tests the null hypothesis that the samples have homegeneous variances. The results of this test (W=2.97, p-value=0.00073) indicates the null hypothesis has to be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(data_income[data_income.income_desc == 'Under 15K']['meat_weight'], \n",
    "             data_income[data_income.income_desc == '15-24K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '25-34K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '35-49K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '50-74K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '75-99K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '100-124K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '125-149K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '150-174K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '175-199K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '200-249K']['meat_weight'],\n",
    "             data_income[data_income.income_desc == '250K+']['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the assumptions are not met, we use the Kruskal-Wallis test (non-parametric alternative of ANOVA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kruskal(data_income[data_income.income_desc == 'Under 15K']['meat_weight'], \n",
    "              data_income[data_income.income_desc == '15-24K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '25-34K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '35-49K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '50-74K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '75-99K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '100-124K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '125-149K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '150-174K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '175-199K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '200-249K']['meat_weight'],\n",
    "              data_income[data_income.income_desc == '250K+']['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the test (H=32.2, p-value=0.00072) show that the null hypothesis can be rejected, meaning there are significant differences between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use again the post-hoc Tukey's HSD method to find out where the differences are. Since the categorial value takes 11 different values, this will perform many tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_income = MultiComparison(data_income['meat_weight'], data_income['income_desc'])\n",
    "mc_results_income = mc_income.tukeyhsd()\n",
    "print(mc_results_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all these tests, only 3 show there are significant differences:\n",
    "* 100-124K vs Under 15K (p-value=0.0178) \n",
    "* 125-149K vs Under 15K (p-value=0.0103)\n",
    "* 75-99K vs Under 15K (p-value=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : Since the assumptions were not met, a Kruskall-Wallis test is conducted. The results of the test (H=32.2, p-value=0.00072) show that the null hypothesis can be rejected, meaning there are significant differences between the groups. The post-hoc Tukey's HSD method is used to find where the differences are. The results state that:\n",
    "   <ul>\n",
    "       <li>the Under 15K group buys more meat than the 100-124K group (difference of means: 0.0419, p-value=0.0178)</li>\n",
    "       <li>the Under 15K group buys more meat than the 125-149K group (difference of means: 0.0422, p-value=0.0103)</li>\n",
    "       <li>the Under 15K group buys more meat than the 75-99K group (difference of means: 0.0431, p-value=0.001)</li>\n",
    "   </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA test for the meat consumption depending on the age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a dataframe with just the age description (our categorial independent variable) and the weight(kg/day) (our continuous dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_age = df_demo_w_meat[['age_desc', 'weight(kg/day)']]\n",
    "data_age = data_age.rename(columns = {'weight(kg/day)':'meat_weight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(data_age.groupby('age_desc')['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our model and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_age = ols('meat_weight ~ C(age_desc)', data=data_age)\n",
    "results_age = model_age.fit()\n",
    "results_age.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the results of the test (F[5.794]=1.783, p-value=0.114) indicate that the groups do not violate the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov_table_age = sm.stats.anova_lm(results_age, typ=2)\n",
    "anova_table(aov_table_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both the eta and omega squares have a low effect size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check assumptions\n",
    "We need to check the normality and the homoscedasticity of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk's test tests the null hypothesis that the residuals are normally distributed. The results of this test (W=0.78, p-value=1.39e-31) indicates the null hypothesis has to be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.shapiro(results_age.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levene test tests the null hypothesis that the samples have homegeneous variances. The results of this test (W=1.55, p-value=0.1708) indicates that the groups do not violate the assumption of homoscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(data_age[data_age.age_desc == '19-24']['meat_weight'], \n",
    "             data_age[data_age.age_desc == '25-34']['meat_weight'],\n",
    "             data_age[data_age.age_desc == '35-44']['meat_weight'],\n",
    "             data_age[data_age.age_desc == '45-54']['meat_weight'],\n",
    "             data_age[data_age.age_desc == '55-64']['meat_weight'],\n",
    "             data_age[data_age.age_desc == '65+']['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the assumptions are not met, we use the Kruskal-Wallis test (non-parametric alternative of ANOVA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kruskal(data_age[data_age.age_desc == '19-24']['meat_weight'], \n",
    "             data_age[data_age.age_desc == '25-34']['meat_weight'],\n",
    "             data_age[data_age.age_desc == '35-44']['meat_weight'],\n",
    "             data_age[data_age.age_desc == '45-54']['meat_weight'],\n",
    "             data_age[data_age.age_desc == '55-64']['meat_weight'],\n",
    "             data_age[data_age.age_desc == '65+']['meat_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the test (H=10.4, p-value=0.066) show that the groups do not violate the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : Since the assumptions were not met, a Kruskall-Wallis test is conducted. The results of the test (H=10.4, p-value=0.066) show that the groups do not violate the null hypothesis. This means the test is inconclusive to determine if the meat consumption differs depending on the age.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization to all categories of food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to generalize these analysis to each category of food, comparing the ratio of products bought in a specific category among different groups (based on their marital status, their income and their age). We apply the Kruskal-Wallis test with the null hypothesis that the amounts of money saved in each category of food are the same across all groups. The results are stored in a dataframe with the statistic, the p-value and whether the null hypothesis is rejected (ie if the p-value is smaller than 0.05) or not. We define two utility functions: *store_results()* outputs the results in the desired way (statistic, p-value, if the null hypothesis can be rejected and the category of food) and *print_statement()* prints the desired statement (descriptive statistics obtained with *summary_cont()* and the results of the statistical test conducted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results, categ):\n",
    "    dict_results = {'Category':categ,\n",
    "                    'Statistic':results[0],\n",
    "                    'p-value':results[1],\n",
    "                    'Reject':(results[1] < 0.05)}\n",
    "    return dict_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the print statement with all the details is quite long, the parameter *show_details* allows to only show a summary version stating for each category if there is a significant difference or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statement(df, results, IV, categ, show_details = False):\n",
    "    if show_details:\n",
    "        print('The descriptive statistics for the %s products are:' %categ.lower())\n",
    "        print(rp.summary_cont(df.groupby(IV)[categ]))\n",
    "        print()\n",
    "        print('The results of the Kruskall-Wallis test for the %s products are:' %categ.lower())\n",
    "        print(results)\n",
    "    if results[1] < 0.05:\n",
    "        print('There is a significant difference among the tested groups for the %s products.' %categ.lower())\n",
    "    else:\n",
    "        print('The groups tested do not violate the null hypothesis for the %s products.' %categ.lower())\n",
    "    print('----------------------------------------------------------')\n",
    "    print()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posthoc_test(df, results, IV):\n",
    "    for categ in results[results.Reject].Category.values:\n",
    "        mc = MultiComparison(df[categ], df[IV])\n",
    "        print('Post-hoc analysis for %s products with Tukey\\'s HSD method:' %categ)\n",
    "        print(mc.tukeyhsd())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_w_ratio = df_demo[[\"household_key\", \"hh_comp_desc\", \"income_desc\", \"age_desc\"]].merge(ratio_df, on=\"household_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_w_ratio.to_csv('ratio_demo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the Kruskal-Wallis tests to find if there are differences among the groups tested and post-hoc tests with the Tukey's HSD method to locate those differences. Note we have enabled the scrolling for the outputs because the print statements can be extremely long (especially for the income and age groups, due to the high number of groups)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Household composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_hh_comp = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in categories: \n",
    "    #Perform the Kruskal-Wallis test\n",
    "    results = stats.kruskal(df_demo_w_ratio[df_demo_w_ratio.hh_comp_desc == 'Adults No Kids'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.hh_comp_desc == 'Single'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.hh_comp_desc == 'With kids'][categ])\n",
    "    #Print statement\n",
    "    print_statement(df_demo_w_ratio, results, 'hh_comp_desc', categ, show_details = True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_hh_comp = results_hh_comp.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_test(df_demo_w_ratio, results_hh_comp, 'hh_comp_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the ratio of products bought in each category of food by various groups (based on the household composition). Significant differences have been found in the following categories:\n",
    "    <ul>\n",
    "      <li>Seafood: statistic=6.72, pvalue=3.4702e-02</li>\n",
    "      <li>Sweet snacks: statistic=32.14, pvalue=1.049e-07</li>\n",
    "      <li>Fruits: statistic=6.64, pvalue=3.623e-02</li>\n",
    "      <li>Vegetables: statistic=13.56, pvalue=1.136e-03</li>\n",
    "      <li>Beverage: statistic=7.20, pvalue=2.737e-02</li>\n",
    "    </ul>\n",
    "    Post-hoc tests have been conducted using the Tukey's HSD method. We report some interesting results (see previous output for an exhaustive list of all the results of the post-hoc tests):\n",
    "    <ul>\n",
    "      <li>single people buy more seafood than people with kids (difference of means: 0.0024, p-value=0.0111)</li>\n",
    "      <li>single people buy more fruits than people with kids (difference of means: 0.0095, p-value=0.0138)</li>\n",
    "      <li>people with no kids buy more vegetables than people with kids (difference of means: 0.0115, p-value=0.0019)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_income = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in categories: \n",
    "    #Perform the Kruskal-Wallis test\n",
    "    results = stats.kruskal(df_demo_w_ratio[df_demo_w_ratio.income_desc == 'Under 15K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '15-24K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '25-34K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '35-49K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '50-74K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '75-99K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '100-124K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '125-149K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '150-174K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '175-199K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '200-249K'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.income_desc == '250K+'][categ])\n",
    "    #Print statement\n",
    "    print_statement(df_demo_w_ratio, results, 'income_desc', categ, show_details = True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_income = results_income.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posthoc_test(df_demo_w_ratio, results_income, 'income_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the amounts of money saved thanks to campaigns by various groups (based on their income) for each category of food. Significant differences have been found in the following categories:\n",
    "    <ul>\n",
    "      <li>Meat: statistic=80.35, pvalue=1.263e-12</li>\n",
    "      <li>Seafood: statistic=20.61, pvalue=3.766e-02</li>\n",
    "      <li>Fruits: statistic=54.03, pvalue=1.167e-07</li>\n",
    "      <li>Vegetables: statistic=21.89, pvalue=2.528e-02\t</li>\n",
    "      <li>Veg animal: statistic=26.94, pvalue=4.693e-03</li>\n",
    "      <li>Beverage: statistic=27.04, pvalue=4.531e-03</li>\n",
    "      <li>Meals: statistic=19.74, pvalue=4.904e-02</li>\n",
    "      <li>Other products: statistic=39.98, pvalue=3.613e-05</li>\n",
    "    </ul>\n",
    "    Post-hoc tests have been conducted using the Tukey's HSD method. We report the most interesting results (see previous output for an exhaustive list of all the results of the post-hoc tests):\n",
    "    <ul>\n",
    "      <li>100-124K group buys more money on seafood than the 25-34K group (difference of means: 0.0068, p-value=0.0408)</li>\n",
    "      <li>150-174K group buys more fruits than the 15-24K group (difference of means: 0.0372, p-value=0.001)</li>\n",
    "      <li>125-149K group saves more money on carbs than the 35-49K group (difference of means: 1.925, p-value=0.0426)</li>\n",
    "      <li>75-99K group saves more money on carbs than the 35-49K group (difference of means: 13.5272, p-value=0.027)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15-24K  150-174K   0.0372  0.001  0.0101  0.0643   True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_income[results_income.Reject]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_age = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in categories: \n",
    "    #Perform the Kruskal-Wallis test\n",
    "    results = stats.kruskal(df_demo_w_ratio[df_demo_w_ratio.age_desc == '19-24'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.age_desc == '25-34'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.age_desc == '35-44'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.age_desc == '45-54'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.age_desc == '55-64'][categ],\n",
    "                            df_demo_w_ratio[df_demo_w_ratio.age_desc == '65+'][categ])\n",
    "    \n",
    "    #Print statement\n",
    "    print_statement(money_saved_demo, results, 'age_desc', categ, show_details = True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_age = results_age.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posthoc_test(df_demo_w_ratio, results_age, 'age_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the amounts of money saved thanks to campaigns by various groups (based on their income) for each category of food. Significant differences have been found in the following categories:\n",
    "    <ul>\n",
    "      <li>Veg animal: statistic=32.16, pvalue=0.00072</li>\n",
    "      <li>Condiments: statistic=26.85, pvalue=0.00485</li>\n",
    "      <li>Carbs: statistic=29.65, pvalue=0.00179</li>\n",
    "      <li>Meals: statistic=25.98, pvalue=0.00654</li>\n",
    "      <li>Other products: statistic=22.03, pvalue=0.02418</li>\n",
    "    </ul>\n",
    "    Post-hoc tests have been conducted using the Tukey's HSD method. We report the most interesting results (i.e. not all the significant results, see previous output for an exhaustive list of all the results of the post-hoc tests):\n",
    "    <ul>\n",
    "      <li>150-174K group saves more money on sweet snacks than the 15-24K group (difference of means: 7.9695, p-value=0.0408)</li>\n",
    "      <li>125-149K group saves more money on carbs than the 15-24K group (difference of means: 2.4332, p-value=0.0092)</li>\n",
    "      <li>125-149K group saves more money on carbs than the 35-49K group (difference of means: 1.925, p-value=0.0426)</li>\n",
    "      <li>75-99K group saves more money on carbs than the 35-49K group (difference of means: 13.5272, p-value=0.027)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:30px 30px; background-color: #3388FF; text-align: center; font-size:30px;\">\n",
    "   <strong>Campaign analysis</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to investigate the campaigns and find out if the products are **equally promoted** or not **across the various categories** of food. If not, this might have an **influence on the consumptions behaviours** of people, encouraging them to buy more products from one category than from another for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coupons distributed and redeemed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking at the number of coupons distributed and redeemed in each category of food, for each campaign. To do sos, we create a function *products_categories()* taking the number of a campaign and plotting the repartition of products for which coupons were offered and were redeemed based on the categories previously defined. It will also return these 2 repartitions as dictionaries with the proportions of each category as values and the categories as keys. A parameter *show_plot* will enable to show or not the plot (if only the proportions are needed). Note that the proportions for the coupons redeemed are computed as the proportion out of the total number of coupons distributed (i.e. 20% coupons redeemed in 'meat' means that 20% of the coupons distributed were redeemed for 'meat'). This is not the same as saying that 20% of the coupons redeemed were for 'meat'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def products_categories(num_camp, show_plot = False):\n",
    "    proportions_dist = dict.fromkeys(categories)\n",
    "    proportions_red = dict.fromkeys(categories)\n",
    "    \n",
    "    #Compute the number of coupons in each category\n",
    "    for categ,df in enumerate(df_list):\n",
    "        #Check if coupon of a category are given for a specific campaign, otherwise simply puts 0 coupons (for coupons distributed)\n",
    "        if num_camp in df.merge(df_coupon, on = 'product_id').groupby(by = 'campaign').count().index:\n",
    "            proportions_dist[categories[categ]] = df.merge(df_coupon, on = 'product_id').groupby(by = 'campaign').count().loc[num_camp][1] \n",
    "        else:\n",
    "            proportions_dist[categories[categ]] = 0\n",
    "        #Check if coupon of a category are given for a specific campaign, otherwise simply puts 0 coupons (for coupons redeemed)    \n",
    "        if num_camp in df.merge(df_coupon[df_coupon.redeemed], on = 'product_id').groupby(by = 'campaign').count().index:  \n",
    "            proportions_red[categories[categ]] = df.merge(df_coupon[df_coupon.redeemed],\n",
    "                                                          on = 'product_id').groupby(by = 'campaign').count().loc[num_camp][1] \n",
    "        else:\n",
    "            proportions_red[categories[categ]] = 0\n",
    "    \n",
    "    #Compute the proportion as percentage\n",
    "        #Compute the total number of coupons distributed\n",
    "    total = 0\n",
    "    for i in proportions_dist.values(): \n",
    "           total += i \n",
    "    if total != 0:\n",
    "        #Compute proportion of coupons distributed per category\n",
    "        for key in proportions_dist.keys():\n",
    "            proportions_dist[key] = 100*proportions_dist[key]/total\n",
    "        #Compute proportion of coupons redeemed per category (proportion of the total number of coupons distributed)\n",
    "            proportions_red[key] = 100*proportions_red[key]/total\n",
    "    \n",
    "    #Plot the repartitions of products for which coupons were offered\n",
    "    if show_plot:   \n",
    "        # set width of bar\n",
    "        barWidth = 0.4\n",
    " \n",
    "        # Set position of bar on X axis\n",
    "        r = np.arange(len(proportions_dist))\n",
    "        r1 = [x - barWidth/2 for x in r]\n",
    "        r2 = [x + barWidth/2 for x in r]\n",
    " \n",
    "        # Make the plot\n",
    "        plt.bar(r1, list(proportions_dist.values()), width=barWidth, edgecolor='white', label='Distributed coupons')\n",
    "        plt.bar(r2, list(proportions_red.values()), width=barWidth, edgecolor='white', label='Redeemed coupons')\n",
    "        plt.xticks(range(len(proportions_dist)), list(proportions_dist.keys()))\n",
    "        plt.title('Proportions of coupons per category during campaign ' + str(num_camp))\n",
    "        plt.xlabel('Categories')\n",
    "        plt.ylabel('Proportion of coupons [%]')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return (proportions_dist, proportions_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the coupons distributed are spread over the different categories for campaign 8 for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prop_dist, prop_red) = products_categories(8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a more overall view, across all campaign. We will compute the proportions of coupons distributed in each category for all campaigns and plot the results in a stacked bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_dist_overall = pd.DataFrame(columns = categories)\n",
    "for i in range(30):\n",
    "    proportions_dist_overall = proportions_dist_overall.append(products_categories(i+1)[0], ignore_index = True)\n",
    "proportions_dist_overall.set_index(df_campaign_desc.index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_dist_overall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors = [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153], [227,26,28], [253,191,111], [255,127,0], [202,178,214], [106,61,154], [255,255,153]]\n",
    "#colormap_ = color.ListedColormap(colors, name='from_list', N=None)\n",
    "proportions_dist_overall.plot.bar(stacked = True)\n",
    "plt.title('Proportions of coupon distributed per category for each campaign')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Proportions of coupon [%]')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.13,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the proportions of coupons redeemed per category for each campaign. Note that in this case the total will not add to 100% as not all coupons are redeemed during a campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_red_overall = pd.DataFrame(columns = categories)\n",
    "for i in range(30):\n",
    "    proportions_red_overall = proportions_red_overall.append(products_categories(i+1)[1], ignore_index = True)\n",
    "proportions_red_overall.set_index(df_campaign_desc.index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_red_overall.plot.bar(stacked = True)\n",
    "plt.title('Proportions of coupon redeemed per category for each campaign')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Proportions of coupon [%]')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.13,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vegetarian vs non-vegetarian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, it is still quite hard to understand what is happening. We will thus group products in fewer categories: vegetarian, non-vegetarian, unknown and other. The 'unknown' category comprises all categories were the exact list of ingredients would be needed to know if it is vegetarian or not. We will only plots the vegetarian and non-vegetarian products as they are the most interesting for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_dist_final = pd.DataFrame()\n",
    "proportions_dist_final['Vegetarian'] = proportions_dist_overall['Fruits'] + proportions_dist_overall['Vegetables'] + proportions_dist_overall['Veg animal'] + proportions_dist_overall['Carbs']\n",
    "proportions_dist_final['Non-vegetarian'] = proportions_dist_overall['Meat'] + proportions_dist_overall['Seafood']\n",
    "proportions_dist_final['Unknown'] = proportions_dist_overall['Sweet snacks'] + proportions_dist_overall['Beverage'] + proportions_dist_overall['Condiments'] + proportions_dist_overall['Meals']\n",
    "proportions_dist_final['Other'] = proportions_dist_overall['Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_dist_final[[\"Vegetarian\", \"Non-vegetarian\"]].plot.bar(stacked = True, color = ['#1b9e77', '#d95f02'])\n",
    "plt.title('Proportions of coupon distributed per category for each campaign')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Proportions of coupon [%]')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.14,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now clear that there are indeed more coupons distributed for vegetarian products than for non-vegetarian ones. Note however that the proportion of coupons for the unknown category is usually the highest, meaning no absolute conclusion can be made here in the vegetarian vs non-vegetarian debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_red_final = pd.DataFrame()\n",
    "proportions_red_final['Vegetarian'] = proportions_red_overall['Fruits'] + proportions_red_overall['Vegetables'] + proportions_red_overall['Veg animal'] + proportions_red_overall['Carbs']\n",
    "proportions_red_final['Non-vegetarian'] = proportions_red_overall['Meat'] + proportions_red_overall['Seafood']\n",
    "proportions_red_final['Unknown'] = proportions_red_overall['Sweet snacks'] + proportions_red_overall['Beverage'] + proportions_red_overall['Condiments'] + proportions_red_overall['Meals']\n",
    "proportions_red_final['Other'] = proportions_red_overall['Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_red_final[[\"Vegetarian\", \"Non-vegetarian\"]].plot.bar(stacked = True, color = ['#1b9e77', '#d95f02'])\n",
    "plt.title('Proportions of coupon redeemed per category for each campaign')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Proportions of coupon [%]')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.14,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the proportion of distributed coupons, the proportion of coupons redeemed for vegetarian products is higher than the one for non-vegetarian products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a more overall view at the vegetarian vs non-vegetarian proportions of coupons distributed and redeemed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "\n",
    "axes[0].boxplot((proportions_dist_final['Vegetarian'],proportions_dist_final['Non-vegetarian']), \n",
    "                labels = ('Vegetarian', 'Non-vegetarian'),\n",
    "                notch = True, bootstrap = 1000, #Add CI for median, computed through bootstrap with n = 1000\n",
    "                widths = 0.6,\n",
    "                showmeans = True,\n",
    "                meanline = True) #Add arithmetic means as dashed line\n",
    "axes[0].set_title('Coupons distributed')\n",
    "axes[1].boxplot((proportions_red_final['Vegetarian'],proportions_red_final['Non-vegetarian']), \n",
    "                labels = ('Vegetarian', 'Non-vegetarian'),\n",
    "                notch = True, bootstrap = 1000, #Add CI for median, computed through bootstrap with n = 1000\n",
    "                widths = 0.6,\n",
    "                showmeans = True,\n",
    "                meanline = True) #Add arithmetic means as dashed line\n",
    "axes[1].set_title('Coupons redeemed')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Category')\n",
    "    ax.set_ylabel('Proportion of coupons')\n",
    "\n",
    "fig.suptitle('Vegetarian vs non-vegetarian proportions of coupons (overall view)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on the vegetarian vs non-vegetarian products debate, the difference over all campaigns is significant. Indeed, the notches show there is a 95% chance that the median proportions of coupons in the two categories do not overlap, both for the coupons distributed and redeemed. Note the mean proportions are also significantly different (green dashed lines). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a small utility function *compute_stats()* to compute the mean of a set of data points and its error (at the confidence level of 0.95 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(data, confidence = 0.95):\n",
    "\n",
    "    n = len(data)\n",
    "    m = scipy.mean(data)\n",
    "    std_err = stats.sem(data)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    return (m, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize our results for the coupons distributed and redeemed in the vegetarian and non-vegetarian categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the coupons distributed and redeemed for the two categories\n",
    "veg_dist = proportions_dist_final['Vegetarian']\n",
    "non_veg_dist = proportions_dist_final['Non-vegetarian']\n",
    "veg_red = proportions_red_final['Vegetarian']\n",
    "non_veg_red = proportions_red_final['Non-vegetarian']\n",
    "\n",
    "#Compute stats for vegetarian products (first element: coupons distributed, second element: coupons redeemed)\n",
    "means_veg = [compute_stats(veg_dist)[0], compute_stats(veg_red)[0]]\n",
    "err_veg = [compute_stats(veg_dist)[1], compute_stats(veg_red)[1]]\n",
    "#Compute stats for non-vegetarian products (first element: coupons distributed, second element: coupons redeemed)\n",
    "means_non_veg = [compute_stats(non_veg_dist)[0], compute_stats(non_veg_red)[0]]\n",
    "err_non_veg = [compute_stats(non_veg_dist)[1], compute_stats(non_veg_red)[1]]\n",
    "\n",
    "#Plot the results\n",
    "barWidth = 0.4\n",
    "r = np.arange(2)\n",
    "r1 = [x - barWidth/2 for x in r]\n",
    "r2 = [x + barWidth/2 for x in r]\n",
    "\n",
    "plt.bar(x = r1, height = means_veg, width = barWidth, yerr = err_veg, label = 'Vegetarian', capsize=10)\n",
    "plt.bar(x = r2, height = means_non_veg, width = barWidth, yerr = err_non_veg, label = 'Non-vegetarian', capsize=10)\n",
    "plt.xticks(np.arange(2), [\"Coupons distributed\", \"Coupons redeemed\"], fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title(\"Proportion of veg vs non-veg coupons (overall view)\", fontsize = 18)\n",
    "plt.ylabel(\"Proportion of coupons [%]\", fontsize = 16)\n",
    "plt.legend(fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform an independent t-test to determine if the differences between the number of coupons distributed for vegetarian and non-vegetarian products are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function *t_test()* taking two parameters, being the two lists of elements we want to compare. In this first case, it would be the proportions of coupons distributed in each campaign for vegetarian and non-vegetarian products.\n",
    "\n",
    "This function first checks the assumptions for the t-test, ie homogeneity of variances (with Levene's test) and normal distribution of residuals (with Shapiro-Wilk's test). If those conditions are met, it proceeds to the t-test using *researchpy.ttest()* method with the parameter *equal_variances* set to True. If the variances are not equal, it tests the normality of the distributions of the two groups (with Shapiro-Wilk's test again) in order to proceeds to the Welch's test (performed by *researchpy.ttest()* with *equal_variances* set to False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(categ1, categ2, visual_norm_check = False):\n",
    "    test_name = ['t-test', 'Welch\\'s t-test', 'Mann-Whitneyu\\'s U-test']\n",
    "    test_index = 0\n",
    "    # == Check assumptions ==\n",
    "    #Homogeneity of variances (Levene's test)\n",
    "    equal_var = True\n",
    "    ass_homo = stats.levene(categ1, categ2)\n",
    "    if ass_homo[1] < 0.05:\n",
    "        #Set equal_variances to False to proceed with Welch's t-test\n",
    "        equal_var = False\n",
    "        test_index = 1\n",
    "        #Test assumptions for Welch's test, ie normality of the distribution of the two groups (Shapiro-Wilk's test)\n",
    "        ass_welch = (stats.shapiro(categ1), stats.shapiro(categ2))\n",
    "    #Normal distribution of residuals (Shapiro-Wilk's test)\n",
    "    residuals = categ1 - categ2\n",
    "    if visual_norm_check:\n",
    "        stats.probplot(residuals, plot= plt)\n",
    "        plt.title('Residuals P-P Plot') \n",
    "        plt.show()\n",
    "        residuals.plot(kind= \"hist\", title= \"Residuals histogram\")\n",
    "        plt.xlabel(\"Residuals\")\n",
    "        plt.show()\n",
    "    ass_norm = stats.shapiro(residuals)\n",
    "    norm_res = True\n",
    "    if ass_norm[1] < 0.05:\n",
    "        #Set norm_res to False and proceed with Mann-Whitneyu's test\n",
    "        norm_res = False\n",
    "        test_index = 2\n",
    "    \n",
    "    # == Run test == (normal t-test or Welch's test, depending on the assumptions that have been checked)\n",
    "    if norm_res:\n",
    "        (test_desc, test_res) = rp.ttest(categ1, categ2, equal_variances=equal_var)\n",
    "    else:\n",
    "        test_mann = stats.mannwhitneyu(categ1, categ2)\n",
    "    \n",
    "    # == Print results == \n",
    "    print('For the Levene\\'s test checking the homogeneity of variances,')\n",
    "    print('we have the following results: [statistic: %0.4f, p-value: %0.6f].' %(ass_homo[0], ass_homo[1]))\n",
    "    if equal_var == False & norm_res == True:\n",
    "        print('Since the variances are not homogeneous, we move to the Welch\\'s test and check for normality of distributions of the two groups.')\n",
    "        print('For the Shapiro-Wilk\\'s test for the first group, we have the following results: [statistic: %0.4f, p-value: %0.6f].' %(ass_welch[0][1], ass_welch[0][1]))\n",
    "        print('For the Shapiro-Wilk\\'s test for the second group, we have the following results: [statistic: %0.4f, p-value: %0.6f].' %(ass_welch[1][1], ass_welch[1][1]))\n",
    "    print('For the Shapiro-Wilk\\'s test checking the normality of residuals,')\n",
    "    print('we have the following results: [statistic: %0.4f, p-value: %0.6f].' %(ass_norm[0], ass_norm[1]))\n",
    "    print('Finally, the results of the %s are the following:' %test_name[test_index])\n",
    "    if norm_res:\n",
    "        print('[df: %0.4f, statistic: %0.4f, p-value: %0.4f]' %(test_res.results[1], test_res.results[2], test_res.results[3]))\n",
    "    else:\n",
    "        print('[statistic: %0.4f, p-value: %0.4f]' %(test_mann[0], test_mann[1]))\n",
    "    \n",
    "    # == Return results == \n",
    "    final_results = [ass_homo, ass_norm]\n",
    "    if norm_res == False:\n",
    "        final_results.append(test_mann)\n",
    "    else:\n",
    "        if equal_var == False:\n",
    "            final_results.append(ass_welch)\n",
    "        final_results.append(test_desc)\n",
    "        final_results.append(test_desc)\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the proportions of coupons distributed for vegetarian and non-vegetarian products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dist = t_test(veg_dist, non_veg_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_red = t_test(veg_red, non_veg_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have analyzed the proportions of coupons distributed and redeemed during the 30 campaigns for vegetarian and non-vegetarian products: \n",
    "        <ul>\n",
    "          <li>coupons distributed: 20.2 $\\pm$ 9.24 % (veg) vs 7.55 $\\pm$ 4.16 % (non-veg)</li>\n",
    "          <li>coupons redeemed: 14.4 $\\pm$ 8.12 % (veg) vs 6.11 $\\pm$ 3.63 % (non-veg)</li>\n",
    "        </ul>\n",
    "    Since the assumptions of normality and homescedasticity were not met, we have conducted a Mann-Whitneyu's U-test (the non-parametric alternative of the independent t-test) to test if the proportions of coupons in the two categories are significantly different. The results are only conclusive for the coupons distributed (p-value smaller than 0.05), meaning there is a significant difference between coupons distributed for vegetarian and non-vegetarian products. This is not the case for the coupons redeemed (p-value bigger than 0.05):\n",
    "        <ul>\n",
    "          <li>coupons distributed: U=297, p-value=0.0117</li>\n",
    "          <li>coupons redeemed: U=370, p-value=0.1162</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much money is actually saved during each campaign?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have shown that campaigns tend to offer more coupons for vegetarian products compared to non-vegetarian ones (the same trend is observed for the number of coupons redeemed). However this comparisons need to also take into account the actual discount offered, as not all coupons offer the same one. It is indeed more relevant to find out how much money households save thanks to the coupons offered during the campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by inspecting the discount values of the coupons. We will add the *coupon_disc* and *coupon_match_disc* values as they both represent a discount due to a coupon (ie money saved by the households thanks to coupons) and take the mean discount values (since coupons might have different discount values depending on the products they are used on). Note we will consider positive values, meaning that if a coupon discount reads -0.4, we will consider its discount value as 0.4. We therefore apply the absolute function to the *coupon_disc* column, after having merged the coupons and the transactions dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_values = df_coupon.merge(df_transaction, on = 'product_id')[['coupon_upc', 'product_id', 'coupon_disc', 'coupon_match_disc']]\n",
    "discount_values = discount_values.groupby('coupon_upc').mean()\n",
    "discount_values = discount_values['coupon_disc'] + discount_values['coupon_match_disc']\n",
    "discount_values = discount_values.map(lambda x: np.abs(x)).sort_values(ascending = False)\n",
    "discount_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have noticed some of the coupons have a null discount value. Let's find out how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_coupons = len(discount_values)\n",
    "coupons_no_discount = len(discount_values[discount_values.values == 0])\n",
    "prop_no_discount = 100*coupons_no_discount/tot_coupons\n",
    "print('There are %d coupons with a null discount value out of %d coupons in total (ie %0.2f%%).' %(coupons_no_discount, tot_coupons, prop_no_discount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, many coupons (a third of them) have no discount values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the distribution, using a log scale in the y axis since it is very right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_values.hist(bins = 100)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Discount value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of the coupons discount values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, not all coupons offer the same discount, meaning that focusing on just the raw number of coupons offered for each category of food is not enough. The actual discount value is more relevant to find out if retailer are promoting some consumption behaviour over the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by merging the transactions and coupons dataframes. We only keep some columns as many of them are not useful for this analysis. As before, we add the discount values from the *coupon_disc* and *coupon_match_disc* columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"product_id\", \"campaign\", \"coupon_disc\", \"redeemed\", \"household_key\"]\n",
    "df_money_offered = df_transaction.merge(df_coupon, on = 'product_id')\n",
    "df_money_offered['coupon_disc'] += df_money_offered['coupon_match_disc']\n",
    "df_money_offered['coupon_disc'] = df_money_offered['coupon_disc'].map(lambda x : np.abs(x))\n",
    "df_money_offered = df_money_offered[cols]\n",
    "df_money_saved = df_money_offered[df_money_offered.redeemed][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_money_offered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the amounts of money offered and saved for each campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set width of bar\n",
    "barWidth = 0.4\n",
    " \n",
    "# set height of bar\n",
    "money_offered = df_money_offered.groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "money_saved = df_money_offered[df_money_offered.redeemed].groupby(by = 'campaign')['coupon_disc'].sum()\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r = np.arange(len(money_offered)+1)\n",
    "r1 = [x - barWidth/2 for x in r[1:]]\n",
    "r2 = [x + barWidth/2 for x in r[1:]]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(r1, money_offered, width=barWidth, edgecolor='white', label='Money offered')\n",
    "plt.bar(r2, money_saved, width=barWidth, edgecolor='white', label='Money saved')\n",
    "plt.yscale('log') #Better visualizations because very different amounts of coupon between campaign\n",
    " \n",
    "# Attach a text label above each bar in *bars*, displaying the campaign type\n",
    "type_c = df_campaign_desc.description.values\n",
    "for i in range(len(coupon_distributed.values)):    \n",
    "    plt.annotate('{}'.format(type_c[i]),\n",
    "                xy=(r[i+1], money_offered.values[i]),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom',\n",
    "                fontweight = 'bold',\n",
    "                fontsize = 8)\n",
    "  \n",
    "    \n",
    "# Add title and legend\n",
    "plt.title('Amounts of money per campaign')\n",
    "plt.xlabel('Campaign')\n",
    "plt.xticks(df_campaign_desc.index)\n",
    "plt.ylabel('Amount of money')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the total amounts of money offered and saved differ a lot across campaigns. This needs to be taken into account when comparing different campaigns. However, in most cases, the amounts of money offered and saved seem to be very close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Money offered per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function *amount_categories()* to compute the amounts of money offered and saved during a given campaign in each category of food. The *show_plot* parameter allows to choose if we want to plot the amounts of money per category. The parameters *normalise_off* and *normalise_sav* allow to choose if we want to compute the raw amount of money or the proportion for each category. Indeed, when focusing on just one campaign the raw amount is enough, but if we want to compare different campaigns we need to look at proportions of money as not all campaigns offered the same amount of money in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amount_categories(num_camp, show_plot = False, normalise_off = False, normalise_sav = False):\n",
    "    amounts_offered = dict.fromkeys(categories) \n",
    "    amounts_saved = dict.fromkeys(categories)\n",
    "    \n",
    "    #Compute the amounts in each category\n",
    "    total_off = 0\n",
    "    total_saved = 0\n",
    "    for categ,df in enumerate(df_list):\n",
    "        #Check if coupon of a category are given for a specific campaign, otherwise simply puts 0 \n",
    "        if num_camp in df_money_offered[df_money_offered.campaign == num_camp].merge(df, on = 'product_id').campaign.values:\n",
    "            amounts_offered[categories[categ]] = df_money_offered[df_money_offered.campaign == num_camp].merge(df, \n",
    "                                                 on = 'product_id').coupon_disc.sum()\n",
    "            total_off += amounts_offered[categories[categ]]\n",
    "        else:\n",
    "            amounts_offered[categories[categ]] = 0\n",
    "        #Check if coupon of a category are given for a specific campaign, otherwise simply puts 0 \n",
    "        if num_camp in df_money_saved[df_money_saved.campaign == num_camp].merge(df, on = 'product_id').campaign.values:\n",
    "            amounts_saved[categories[categ]] = df_money_saved[df_money_saved.campaign == num_camp].merge(df, \n",
    "                                                 on = 'product_id').coupon_disc.sum()\n",
    "            total_saved += amounts_saved[categories[categ]]\n",
    "        else:\n",
    "            amounts_saved[categories[categ]] = 0\n",
    "\n",
    "    #Normalisation\n",
    "    if normalise_off and total_off != 0:\n",
    "        for categ in categories:\n",
    "            amounts_offered[categ] = 100*amounts_offered[categ]/total_off\n",
    "    if normalise_sav and total_saved != 0:\n",
    "        for categ in categories:\n",
    "            amounts_saved[categ] = 100*amounts_saved[categ]/total_saved\n",
    "\n",
    "    #Plot the amount of money in each category\n",
    "    if show_plot:   \n",
    "        # set width of bar\n",
    "        barWidth = 0.4\n",
    " \n",
    "        # Set position of bar on X axis\n",
    "        r = np.arange(len(amounts_offered))\n",
    "        r1 = [x - barWidth/2 for x in r]\n",
    "        r2 = [x + barWidth/2 for x in r]\n",
    " \n",
    "        # Make the plot\n",
    "        plt.bar(r1, list(amounts_offered.values()), width=barWidth, edgecolor='white', label='Money offered')\n",
    "        plt.bar(r2, list(amounts_saved.values()), width=barWidth, edgecolor='white', label='Money saved')\n",
    "        plt.xticks(range(len(amounts_offered)), list(amounts_offered.keys()))\n",
    "        if normalise_off:\n",
    "            plt.title('Proportions of money offered per category during campaign ' + str(num_camp))\n",
    "            plt.ylabel('Proportion of money offered [%]')\n",
    "        else:\n",
    "            plt.title('Amounts of money offered per category during campaign ' + str(num_camp))\n",
    "            plt.ylabel('Amount of money offered')            \n",
    "        plt.xlabel('Categories')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return (amounts_offered, amounts_saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our fonction on campaign n°2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amounts_test = amount_categories(2, True, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the total discount obtained with coupons for each campaign (looking at each category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_money_offered_categ = pd.DataFrame()\n",
    "df_money_offered_categ['Total'] = df_money_offered.groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "for categ,df in enumerate(df_list):\n",
    "    df_money_offered_categ[categories[categ]] = df_money_offered.merge(df, on = 'product_id').groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "df_money_offered_categ['Vegetarian'] = df_money_offered.merge(df_veg, on = 'product_id')[cols].groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "df_money_offered_categ['Non-vegetarian'] = df_money_offered.merge(df_non_veg, on = 'product_id')[cols].groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "df_money_offered_categ.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(df_money_offered_categ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise the amounts of money offered to better compare two campaigns (as we have seen that the total amount of money offered can differ a lot). We also add the amounts normalised for the vegetarian and non-vegetarian categories. Note we directly compute the amount of money saved that will be used later (better efficiency to compute both at once). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_money_offered_categ_normalized = pd.DataFrame(columns = categories)\n",
    "df_money_saved_categ_normalized = pd.DataFrame(columns = categories)\n",
    "for i in range(30):\n",
    "    temp = amount_categories(i+1, False, True, True)\n",
    "    df_money_offered_categ_normalized = df_money_offered_categ_normalized.append(temp[0], ignore_index = True)\n",
    "    df_money_saved_categ_normalized = df_money_saved_categ_normalized.append(temp[1], ignore_index = True)\n",
    "df_money_offered_categ_normalized.set_index(df_campaign_desc.index, inplace = True)\n",
    "df_money_saved_categ_normalized.set_index(df_campaign_desc.index, inplace = True)\n",
    "\n",
    "df_money_offered_categ_normalized['Vegetarian'] = df_money_offered_categ_normalized['Fruits'] + df_money_offered_categ_normalized['Vegetables'] + df_money_offered_categ_normalized['Carbs'] + df_money_offered_categ_normalized['Veg animal']\n",
    "df_money_offered_categ_normalized['Non-vegetarian'] = df_money_offered_categ_normalized['Meat'] + df_money_offered_categ_normalized['Seafood']\n",
    "df_money_saved_categ_normalized['Vegetarian'] = df_money_saved_categ_normalized['Fruits'] + df_money_saved_categ_normalized['Vegetables'] + df_money_saved_categ_normalized['Carbs'] + df_money_saved_categ_normalized['Veg animal']\n",
    "df_money_saved_categ_normalized['Non-vegetarian'] = df_money_saved_categ_normalized['Meat'] + df_money_saved_categ_normalized['Seafood']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our results in a barplot (first for each category, then for veg vs non-veg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_money_offered_categ_normalized[categories].plot.bar(stacked = True)\n",
    "plt.title('Amount of money offered per category for each campaign')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.xlabel('Campaign')\n",
    "plt.ylabel('Amount of money')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.13,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_money_offered_categ_normalized[[\"Vegetarian\", \"Non-vegetarian\"]].plot.bar(stacked = True)\n",
    "plt.title('Proportions of money offered for each campaign (veg vs non-veg)')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.xlabel('Campaign')\n",
    "plt.ylabel('Proportion of money [%]')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.14,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the proportion of money offered for vegetarian products seem higher than for non-vegetarian products in the majority of campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarise those results for the vegetarian and non-vegetarian categories in a boxplot. Note we have decided to not show outliers as they would squeeze too much the boxes. We display both the raw amounts of money and the proportions of money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "\n",
    "axes[0].boxplot((df_money_offered_categ[\"Vegetarian\"],df_money_offered_categ[\"Non-vegetarian\"]), \n",
    "                labels = ('Vegetarian', 'Non-vegetarian'),\n",
    "                notch = True, bootstrap = 1000, #Add CI for median, computed through bootstrap with n = 1000\n",
    "                widths = 0.6,\n",
    "                showfliers = False) \n",
    "axes[0].set_ylabel('Amount of money')\n",
    "axes[0].set_title('Total amount of money offered with coupons per category')\n",
    "\n",
    "axes[1].boxplot((df_money_offered_categ_normalized[\"Vegetarian\"],df_money_offered_categ_normalized[\"Non-vegetarian\"]), \n",
    "                labels = ('Vegetarian', 'Non-vegetarian'),\n",
    "                notch = True, bootstrap = 1000, #Add CI for median, computed through bootstrap with n = 1000\n",
    "                widths = 0.6,\n",
    "                showfliers = False)\n",
    "axes[1].set_ylabel('Proportion of money [%]')\n",
    "axes[1].set_title('Total proportion of money offered with coupons per category')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Category')\n",
    "\n",
    "fig.suptitle('Vegetarian vs non-vegetarian amounts of money offered (overall view)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Money saved per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the coupons redeemed, thus getting the actual amount of money saved by customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_money_saved_categ = pd.DataFrame()\n",
    "df_money_saved_categ['Total'] = df_money_saved.groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "for categ,df in enumerate(df_list):\n",
    "    df_money_saved_categ[categories[categ]] = df_money_saved.merge(df, on = 'product_id').groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "df_money_saved_categ['Vegetarian'] = df_money_saved.merge(df_veg, on = 'product_id')[cols].groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "df_money_saved_categ['Non-vegetarian'] = df_money_saved.merge(df_non_veg, on = 'product_id')[cols].groupby(by = 'campaign')['coupon_disc'].sum()\n",
    "df_money_saved_categ.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.summary_cont(df_money_saved_categ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our results in a barplot (first for each category, then for veg vs non-veg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_money_saved_categ_normalized[categories].plot.bar(stacked = True)\n",
    "plt.title('Amount of money saved per category for each campaign')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.xlabel('Campaign')\n",
    "plt.ylabel('Amount of money')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.13,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_money_saved_categ_normalized[[\"Vegetarian\", \"Non-vegetarian\"]].plot.bar(stacked = True)\n",
    "plt.title('Amount of money saved for each campaign (veg vs non-veg)')\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.xlabel('Campaign')\n",
    "plt.ylabel('Amount of money')\n",
    "plt.legend(loc = 5, bbox_to_anchor = (1.14,0.77))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarise those results for the vegetarian and non-vegetarian categories in a boxplot. Here again, we have decided to not show outliers as they would squeeze too much the boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "\n",
    "axes[0].boxplot((df_money_saved_categ[\"Vegetarian\"],df_money_saved_categ[\"Non-vegetarian\"]), \n",
    "                labels = ('Vegetarian', 'Non-vegetarian'),\n",
    "                notch = True, bootstrap = 1000, #Add CI for median, computed through bootstrap with n = 1000\n",
    "                widths = 0.6,\n",
    "                showfliers = False) \n",
    "axes[0].set_ylabel('Amount of money')\n",
    "axes[0].set_title('Total amount of money saved with coupons per category')\n",
    "\n",
    "axes[1].boxplot((df_money_saved_categ_normalized[\"Vegetarian\"],df_money_saved_categ_normalized[\"Non-vegetarian\"]), \n",
    "                labels = ('Vegetarian', 'Non-vegetarian'),\n",
    "                notch = True, bootstrap = 1000, #Add CI for median, computed through bootstrap with n = 1000\n",
    "                widths = 0.6,\n",
    "                showfliers = False)\n",
    "axes[1].set_ylabel('Proportion of money [%]')\n",
    "axes[1].set_title('Total proportion of money saved with coupons per category')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Category')\n",
    "\n",
    "fig.suptitle('Vegetarian vs non-vegetarian amounts of money saved (overall view)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final picture in the veg vs non-veg debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize our results for the vegetarian and non-vegetarian categories and run the appropriate statistical tests to find out if the differences observed are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the amounts of money offered and saved for the two categories\n",
    "veg_off = df_money_offered_categ_normalized[\"Vegetarian\"]\n",
    "non_veg_off = df_money_offered_categ_normalized[\"Non-vegetarian\"]\n",
    "veg_sav = df_money_saved_categ_normalized[\"Vegetarian\"]\n",
    "non_veg_sav = df_money_saved_categ_normalized[\"Non-vegetarian\"]\n",
    "\n",
    "#Compute stats for vegetarian products (first element: money offered, second element: money saved)\n",
    "money_means_veg = [compute_stats(veg_off)[0], compute_stats(veg_sav)[0]]\n",
    "money_err_veg = [compute_stats(veg_off)[1], compute_stats(veg_sav)[1]]\n",
    "#Compute stats for non-vegetarian products (first element: money offered, second element: money saved)\n",
    "money_means_non_veg = [compute_stats(non_veg_off)[0], compute_stats(non_veg_sav)[0]]\n",
    "money_err_non_veg = [compute_stats(non_veg_off)[1], compute_stats(non_veg_sav)[1]]\n",
    "\n",
    "#Plot the results\n",
    "barWidth = 0.4\n",
    "r = np.arange(2)\n",
    "r1 = [x - barWidth/2 for x in r]\n",
    "r2 = [x + barWidth/2 for x in r]\n",
    "\n",
    "plt.bar(x = r1, height = money_means_veg, yerr = money_err_veg, width = barWidth, label = 'Vegetarian', capsize=10)\n",
    "plt.bar(x = r2, height = money_means_non_veg, yerr = money_err_non_veg, width = barWidth, label = 'Non-vegetarian', capsize=10)\n",
    "plt.xticks(np.arange(2), [\"Money offered\", \"Money saved\"], fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title(\"Proportion of money for veg vs non-veg products (overall view)\", fontsize = 18)\n",
    "plt.ylabel(\"Proportion of money [%]\", fontsize = 16)\n",
    "plt.legend(fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, since we are looking at proportions of money, the fact that the money saved is more than the money offered is not strange. This could happens for instance if only the higher discount values coupons were redeemed (imagine you have nine 1€-coupons and one 10€-coupon, if the 10€-coupon is the only one redeemed, the mean amount of money saved will be 10€ while the mean amount of money offered is only 1.9€)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_off = t_test(df_money_offered_categ_normalized[\"Vegetarian\"],df_money_offered_categ_normalized[\"Non-vegetarian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sav = t_test(df_money_saved_categ_normalized[\"Vegetarian\"],df_money_saved_categ_normalized[\"Non-vegetarian\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have analyzed the proportions of money offered and saved during the 30 campaigns for vegetarian and non-vegetarian products: \n",
    "        <ul>\n",
    "          <li>money offered: 22.85 $\\pm$ 9.69 % (veg) vs 5.74 $\\pm$ 3.01 % (non-veg)</li>\n",
    "          <li>money saved: 21.84 $\\pm$ 10.06 % (veg) vs 8.34 $\\pm$ 7.12 % (non-veg)</li>\n",
    "        </ul>\n",
    "    Since the assumptions of normality and homescedasticity were not met, we have conducted a Mann-Whitneyu's U-test (the non-parametric alternative of the independent t-test) to test if the proportions of money in the two categories are significantly different. The results are conclusive, as both p-value are smaller than 0.05, meaning the groups tested are significantly different:\n",
    "        <ul>\n",
    "          <li>money offered: U=268.5, p-value=0.0033</li>\n",
    "          <li>money saved: U=331.5, p-value=0.0344</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before & after a campaign\n",
    "Let's compare consumption behaviours before and after each campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we design a function taking the id of a campaign (*num_camp*), the *df* and *categ* of the category of food we are interested in and *show_plot* to specify if we want to plot the results. It computes the sales before and after the given campaign in the given category, then gets the means and respective 95% CI. Finally, it tests the null hypothesis H0 that the sales before and after the campaign have the same mean with the Mann-Whiteneyu's non-parametric test, at the $\\alpha$ level given (0.05 by default). Note the sales values are normalized per day since the number of days before and after a given campaign is not always the same (especially for the first and last campaigns) and with the total sales values (to get the proportion of sales values in the given category). If there are not at least 10 days before or after, all results are mapped to -1 (impossible value to get otherwise) to easily remove them later and the function outputs that the results are not significant (to identify which campaigns follow this behaviour).\n",
    "\n",
    "Note we have decided to split this function in smaller ones, defining *get_sales()* to compute the normalized sales before and after, *compute_test()* to compute the corresponding means and 95% (by default) confidence interval for the means and finally *mann_test()* to perform the Mann-Whitneyu's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sales(num_camp, df):\n",
    "    #Obtain starting and ending day of the given campaign (+duration to normalise sales values per day)\n",
    "    start_day = df_campaign_desc.loc[num_camp].start_day\n",
    "    end_day = df_campaign_desc.loc[num_camp].end_day\n",
    "    before = start_day - 1  #Number of days before the campaign\n",
    "    after = 711 - end_day  #Number of days after the campaign  \n",
    "    \n",
    "    #Compute the sales before and after the campaign for a specific category\n",
    "    significant = True\n",
    "    sales_in_category = df_transaction[df_transaction.product_id.isin(df.product_id)]\n",
    "    if before > 10:\n",
    "        #Compute sales in the category and total sales for each household\n",
    "        sales_before = sales_in_category[sales_in_category.day < start_day].groupby(by = 'household_key').sales_value.sum()\n",
    "        tot_sales_before = df_transaction[df_transaction.day < start_day].groupby(by = 'household_key').sales_value.sum()\n",
    "        #Keep only the households that have bought some products in the given category\n",
    "        tot_sales_before = tot_sales_before[tot_sales_before.index.isin(sales_before.index)]\n",
    "        #Divide the sales in the category by the total sales (to get the proportion) and normalize per number of days\n",
    "        sales_before = sales_before.divide(tot_sales_before)\n",
    "        sales_before = sales_before.map(lambda x: 100*x/before)\n",
    "    else:\n",
    "        sales_before = -1\n",
    "        significant = False\n",
    "    if after > 10:\n",
    "        #Compute sales in the category and total sales for each household\n",
    "        sales_after = sales_in_category[sales_in_category.day > end_day].groupby(by = 'household_key').sales_value.sum()\n",
    "        tot_sales_after = df_transaction[df_transaction.day > end_day].groupby(by = 'household_key').sales_value.sum()\n",
    "        #Keep only the households that have bought some products in the given category\n",
    "        tot_sales_after = tot_sales_after[tot_sales_after.index.isin(sales_after.index)]\n",
    "        #Divide the sales in the category by the total sales (to get the proportion) and normalize per number of days\n",
    "        sales_after = sales_after.divide(tot_sales_after)\n",
    "        sales_after = sales_after.map(lambda x: 100*x/after)\n",
    "    else:\n",
    "        sales_after = -1\n",
    "        significant = False\n",
    "        \n",
    "    return (sales_before, sales_after, significant)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(data, confidence = 0.95):\n",
    "\n",
    "    n = len(data)\n",
    "    m = scipy.mean(data)\n",
    "    std_err = stats.sem(data)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    return (m, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mann_test(data1, data2, show_test, alpha):\n",
    "    results = stats.mannwhitneyu(data1, data2)\n",
    "    if show_test:\n",
    "        print(\"Results of the Mann-Whitneyu\\'s test: statistic U: %0.3f, p-value: %f\" %(results[0], results[1]))  \n",
    "        if results[1] < alpha:\n",
    "            print(\"The p-value is smaller than the \\u03B1 level given: the null hypothesis can be rejected.\")\n",
    "        else:\n",
    "            print(\"The p-value is bigger than the \\u03B1 level given: the groups tested do not violate the null hypothesis.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_after_sales(num_camp, df, categ = \"\", show_plot = False, show_test = False, alpha = 0.05):\n",
    "    #Compute the sales before and after the campaign for a specific category\n",
    "    (sales_before, sales_after, significant) = get_sales(num_camp, df)\n",
    "    \n",
    "    #Compute the mean and 95% CI of the mean for the sales\n",
    "    if significant:\n",
    "        sales_stats = (compute_stats(sales_before), compute_stats(sales_after))\n",
    "        means = [sales_stats[0][0], sales_stats[1][0]]\n",
    "        err = [sales_stats[0][1], sales_stats[1][1]]\n",
    "    else:\n",
    "        means = [-1, -1]\n",
    "        err = [-1, -1]\n",
    "    \n",
    "    #Plot the mean sales values before and after the campaign\n",
    "    if significant:\n",
    "        if show_plot:\n",
    "            barWidth = 0.4\n",
    "            plt.bar(x = np.arange(2), height = means, width = barWidth, yerr = err, capsize=10)\n",
    "            plt.xticks(np.arange(2), [\"Before\", \"After\"], fontsize = 16)\n",
    "            plt.yticks(fontsize = 16)\n",
    "            plt.title(\"Proportion of %s sales values per day before and after campaign %d\" %(categ, num_camp), fontsize = 18)\n",
    "            plt.ylabel(\"Proportion of %s sales values per day [%%]\" %categ, fontsize = 16)\n",
    "            plt.show()\n",
    "            \n",
    "    #Specify if the results are not significant due to low number of days before or after the campaign\n",
    "    if significant == False:\n",
    "        print(\"The results for campaign %d are not significant because the number of days before or after the campaign is too low.\" %num_camp)\n",
    "        \n",
    "    #t-test\n",
    "    if significant:\n",
    "        results = mann_test(sales_before, sales_after, show_test, alpha)\n",
    "    else:\n",
    "        results = (-1, -1)\n",
    "                \n",
    "    return (means, err, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with campaign 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(means, err, results) = before_after_sales(6, df_meat, \"Meat\", True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there is an increase in the proportion of sales values of non-vegetarian products after campaign 6. The difference is significant, due to the results of the Mann-Whiteneyu's test (U = 2102870, p-value << 0.05). This is quite surprising because campaign 6 was only about vegetarian products and therefore we expected that it would have encouraged people to buy less non-vegetarian products!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run this function for each campaign. We write a function that simply takes the category we are interested in and the $\\alpha$ level for the statistical test. We store in a dataframe the means with the errors (for a confidence level of 95%) as well as the results of the Mann-Whitneyu's test for each campaign and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_after_overall_view(df, categ, alpha = 0.05):\n",
    "    #Computes the sales before and after for each campaign\n",
    "    sales = pd.DataFrame(columns = [\"Campaign\", \"Mean before\", \"Mean after\", \"Error before\", \"Error after\", \"U stat\", \"p-value\"])\n",
    "    for i in range(30):\n",
    "        (means, err, results) = before_after_sales(i+1, df)\n",
    "        sales = sales.append({\"Campaign\": i+1,\n",
    "                              \"Mean before\": means[0],\n",
    "                              \"Mean after\": means[1],\n",
    "                              \"Error before\": err[0],\n",
    "                              \"Error after\": err[1],\n",
    "                              \"U stat\": results[0],\n",
    "                              \"p-value\": results[1],\n",
    "                              \"Reject\": (results[1] < 0.05)}, #Specify if the null hypothesis is rejected (ie there is a significant difference)\n",
    "                              ignore_index = True)\n",
    "    #Discard the non-significant campaigns (values set to -1 in the before_after_sales() function)\n",
    "    sales = sales[sales['Mean before'] > 0]\n",
    "    sales['Reject'] = sales['Reject'].map({0:False, 1:True})\n",
    "    \n",
    "    #Plot the results\n",
    "    barWidth = 0.4\n",
    "    r = np.arange(len(sales))\n",
    "    r1 = [x - barWidth/2 for x in r]\n",
    "    r2 = [x + barWidth/2 for x in r]\n",
    "\n",
    "    err = sales[[\"Error before\", \"Error after\"]].values\n",
    "    plt.bar(x = r1, height = sales['Mean before'], width = barWidth,\n",
    "                    yerr =sales['Error before'], label = 'Before', capsize=10)\n",
    "    plt.bar(x = r2, height = sales['Mean after'], width = barWidth, \n",
    "                    yerr = sales['Error after'], label = 'After', capsize=10)\n",
    "    plt.xticks(np.arange(len(sales)), sales.Campaign.map(lambda x: int(x)).values, rotation = 'horizontal', fontsize = 16)\n",
    "    plt.xlabel('Campaign', fontsize = 16)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    plt.ylabel('Proportion of sales values per day [%]', fontsize = 16)\n",
    "    plt.title('Proportions of sales values per day for %s for each campaign' %categ, fontsize = 18)\n",
    "    plt.legend(fontsize = 16)\n",
    "    plt.show()\n",
    "    \n",
    "    #Return the sales dataframe if needed\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function for the meat consumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meat_sales = before_after_overall_view(df_meat, \"meat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if any of the tests conducted were not conclusive. It seems this is the case only for campaign 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meat_sales[meat_sales.Reject == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have analyzed the proportions of sales values per day for meat before and after each campaign to find out if the promoting periods had an impact on consumptions behaviours. Since the assumptions of normality and homescedasticity were not met, we have conducted a Mann-Whitneyu's U-test (the non-parametric alternative of the independent t-test) to test if the proportions of sales before and after each campaign are significantly different. The results are conclusive for all campaigns except campaigns 1 and 2 (both p-value are bigger than 0.05) and for campaigns 15 and 24 (not enough days before or after), meaning the groups tested are significantly different.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for vegetarian products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_sales = before_after_overall_view(df_veg, \"vegetarian products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if any of the tests conducted were not conclusive. It seems the results are conclusive for all campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_sales[veg_sales.Reject == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have analyzed the proportions of sales values per day for vegetarian products before and after each campaign to find out if the promoting periods had an impact on consumptions behaviours. Since the assumptions of normality and homescedasticity were not met, we have conducted a Mann-Whitneyu's U-test (the non-parametric alternative of the independent t-test) to test if the proportions of sales before and after each campaign are significantly different. The results are conclusive for all campaigns except campaigns 15 and 24 (not enough days before or after), meaning the groups tested are significantly different.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response of the households given the demographics information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we want to find out if different groups of people are impacted differently by the promoting campaigns. To do so, we combine the demographics informations we have on 801 households (age, income and household composition) with the informations about the campaigns (coupons, amount of money offered/saved). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_per_household_per_category = pd.DataFrame()\n",
    "#Let's add the money saved in total\n",
    "money_per_household_per_category['Total'] = df_transaction.groupby(by = 'household_key').coupon_disc.sum() \n",
    "#Let's add the money saved for products in each category\n",
    "for (categ,df) in enumerate(df_list):\n",
    "    money_per_household_per_category[categories[categ]] = df_transaction[df_transaction.product_id.isin(df.product_id)].groupby(by = 'household_key').coupon_disc.sum()\n",
    "money_per_household_per_category.fillna(0, inplace = True)\n",
    "money_per_household_per_category = money_per_household_per_category.apply(lambda x: np.abs(x))\n",
    "#Let's add the money saved for vegetarian and non-vegetarian products\n",
    "money_per_household_per_category['Vegetarian'] = money_per_household_per_category['Fruits'] + money_per_household_per_category['Vegetables'] + money_per_household_per_category['Veg animal'] + money_per_household_per_category['Carbs']\n",
    "money_per_household_per_category['Non-vegetarian'] = money_per_household_per_category['Meat'] + money_per_household_per_category['Seafood']\n",
    "money_per_household_per_category.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_saved_demo = money_per_household_per_category[money_per_household_per_category.household_key.isin(df_demo.household_key)]\n",
    "money_saved_demo.reset_index(inplace = True, drop = True)\n",
    "money_saved_demo['age_desc'] = df_demo['age_desc']\n",
    "money_saved_demo['income_desc'] = df_demo['income_desc']\n",
    "money_saved_demo['hh_comp_desc'] = df_demo['hh_comp_desc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the results, we use the function *plot_categ_groups()* previously defined. Let's have an example for the meat consumption given the household composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categ_groups(money_saved_demo, 'Meat', 'hh_comp_desc',\n",
    "                  'Money saved', 'Money saved for meat products by groups based on the household composition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at the amounts of money saved per category of food for a specific household (here the first one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_saved_demo[categories].iloc[0].plot.bar()\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Amount of money')\n",
    "plt.title('Amounts of money saved per category for the given household')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the general behaviour of the households. How many households have not saved any money?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%0.2f%% households have not saved any money.' %(100*money_saved_demo[money_saved_demo.Total == 0].Total.count()/money_saved_demo.Total.count()))\n",
    "filter = np.logical_or(money_saved_demo.Vegetarian == 0, money_saved_demo['Non-vegetarian'] == 0)\n",
    "print('Only %0.2f%% households have saved money for vegetarian or non-veg products.' %(100*money_saved_demo[filter].Total.count()/money_saved_demo.Total.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical tests for the 11 categories of food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the amounts of money saved by different groups of households (based on the household composition, their income and their age), for the different categories of food. We apply the Kruskal-Wallis test with the null hypothesis that the amounts of money saved in each category of food are the same across all groups. The results are stored in a dataframe with the statistic, the p-value and whether the null hypothesis is rejected (ie if the p-value is smaller than 0.05) or not. We use the previously defined functions *store_results()* to output the results in the desired way (statistic, p-value, if the null hypothesis can be rejected and the category of food) and *print_statement()* to print the desired statement (descriptive statistics obtained with *summary_cont()* and the results of the statistical test conducted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Household composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_hh_comp = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in categories: \n",
    "    #Perform the Kruskal-Wallis test \n",
    "    results = stats.kruskal(money_saved_demo[money_saved_demo.hh_comp_desc == 'Adults No Kids'][categ],\n",
    "                                    money_saved_demo[money_saved_demo.hh_comp_desc == 'Single'][categ],\n",
    "                                    money_saved_demo[money_saved_demo.hh_comp_desc == 'With kids'][categ])\n",
    "    #Print statement\n",
    "    print_statement(money_saved_demo, results, 'hh_comp_desc', categ, show_details = True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_hh_comp = results_hh_comp.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are some significant differences between the tested groups among the different categories of food. We will use the post-hoc Tukey's HSD method to find where those differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posthoc_test(money_saved_demo, results_hh_comp, 'hh_comp_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the amounts of money saved thanks to campaigns by various groups (based on their marital status) for each category of food. Significant differences have been found in the following categories:\n",
    "    <ul>\n",
    "      <li>Sweet snacks: statistic=7.70, p-value=0.0213</li>\n",
    "      <li>Veg animal: statistic=14.7, p-value=0.0007</li>\n",
    "      <li>Beverage: statistic=10.5, p-value=0.0052</li>\n",
    "      <li>Condiments: statistic=9.21, p-value=0.0100</li>\n",
    "      <li>Carbs: statistic=17.2, p-value=0.0002</li>\n",
    "      <li>Meals: statistic=17.7, p-value=0.0001</li>\n",
    "      <li>Other products: statistic=11.2, p-value=0.0037</li>\n",
    "    </ul>\n",
    "    Post-hoc tests have been conducted using the Tukey's HSD method. We report the most interesting results (i.e. not all the significant results, see previous output for an exhaustive list of all the results of the post-hoc tests):\n",
    "    <ul>\n",
    "      <li>Married people save more money on sweet snacks than single people (difference of means: 2.9618, p-value=0.0334)</li>\n",
    "      <li>Married people save more money on beverages than single people (difference of means: 1.4986, p-value=0.0269)</li>\n",
    "      <li>Married people save more money on carbs than single people (difference of means: 0.8234, p-value=0.0458)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the amounts of money saved by different groups of households based on their income, for the different categories of food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_income = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in categories: \n",
    "    #Perform the Kruskal-Wallis test\n",
    "    results = stats.kruskal(money_saved_demo[money_saved_demo.income_desc == 'Under 15K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '15-24K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '25-34K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '35-49K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '50-74K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '75-99K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '100-124K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '125-149K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '150-174K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '175-199K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '200-249K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '250K+'][categ])\n",
    "    \n",
    "    #Print statement\n",
    "    print_statement(money_saved_demo, results, 'income_desc', categ, show_details=True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_income = results_income.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posthoc_test(money_saved_demo, results_marital, 'income_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the amounts of money saved thanks to campaigns by various groups (based on their income) for each category of food. Significant differences have been found in the following categories:\n",
    "    <ul>\n",
    "      <li>Veg animal: statistic=32.16, pvalue=0.00072</li>\n",
    "      <li>Condiments: statistic=26.85, pvalue=0.00485</li>\n",
    "      <li>Carbs: statistic=29.65, pvalue=0.00179</li>\n",
    "      <li>Meals: statistic=25.98, pvalue=0.00654</li>\n",
    "      <li>Other products: statistic=22.03, pvalue=0.02418</li>\n",
    "    </ul>\n",
    "    Post-hoc tests have been conducted using the Tukey's HSD method. We report the most interesting results (i.e. not all the significant results, see previous output for an exhaustive list of all the results of the post-hoc tests):\n",
    "    <ul>\n",
    "      <li>150-174K group saves more money on sweet snacks than the 15-24K group (difference of means: 7.9695, p-value=0.0408)</li>\n",
    "      <li>125-149K group saves more money on carbs than the 15-24K group (difference of means: 2.4332, p-value=0.0092)</li>\n",
    "      <li>125-149K group saves more money on carbs than the 35-49K group (difference of means: 1.925, p-value=0.0426)</li>\n",
    "      <li>75-99K group saves more money on carbs than the 35-49K group (difference of means: 13.5272, p-value=0.027)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the amounts of money saved by different groups of households based on their age, for the different categories of food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_age = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in categories: \n",
    "    #Perform the Kruskal-Wallis test\n",
    "    results = stats.kruskal(money_saved_demo[money_saved_demo.age_desc == '19-24'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '25-34'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '35-44'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '45-54'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '55-64'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '65+'][categ])\n",
    "    \n",
    "    #Print statement\n",
    "    print_statement(money_saved_demo, results, 'age_desc', categ, show_details = True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_age = results_age.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posthoc_test(money_saved_demo, results_age, 'age_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the amounts of money saved thanks to campaigns by various groups (based on their age) for each category of food. Significant differences have been found in the following categories:\n",
    "    <ul>\n",
    "      <li>Veg animal: statistic=14.39, pvalue=0.01333</li>\n",
    "      <li>Beverage: statistic=14.30, pvalue=0.01382</li>\n",
    "    </ul>\n",
    "    Unfortunately, post-hoc tests conducted using the Tukey's HSD method do not show any significant differences between the groups observed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical tests for the vegetarian vs non-vegetarian debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same procedure (Kruskal-Wallis test to identify if there is a significant difference and Tukey's HSD method for post-hoc tests) to the vegetarian and non-vegetarian products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Household composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_debate = ['Vegetarian', 'Non-vegetarian']\n",
    "results_veg_debate_hh_comp = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in veg_debate: \n",
    "    #Perform the Kruskal-Wallis test \n",
    "    results = stats.kruskal(money_saved_demo[money_saved_demo.hh_comp_desc == 'Adults No Kids'][categ],\n",
    "                                    money_saved_demo[money_saved_demo.hh_comp_desc == 'Single'][categ],\n",
    "                                    money_saved_demo[money_saved_demo.hh_comp_desc == 'With kids'][categ])\n",
    "    #Print statement\n",
    "    print_statement(money_saved_demo, results, 'hh_comp_desc', categ, show_details = True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_veg_debate_hh_comp = results_veg_debate_hh_comp.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_test(money_saved_demo, results_veg_debate_hh_comp, 'hh_comp_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the amounts of money saved thanks to campaigns by various groups (based on their age) for vegetarian and non-vegetarian products. Significant differences have been found in both categories:\n",
    "    <ul>\n",
    "      <li>Vegetarian: statistic=15.20, p-value=0.00050</li>\n",
    "      <li>Non-vegetarian: 6.38, pvalue=0.04119</li>\n",
    "    </ul>\n",
    "    Post-hoc tests conducted using the Tukey's HSD method show significant differences between married and unknown marital status people for vegetarian products (p-value=0.001)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_veg_debate_income = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in veg_debate: \n",
    "    #Perform the Kruskal-Wallis test\n",
    "    results = stats.kruskal(money_saved_demo[money_saved_demo.income_desc == 'Under 15K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '15-24K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '25-34K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '35-49K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '50-74K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '75-99K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '100-124K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '125-149K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '150-174K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '175-199K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '200-249K'][categ],\n",
    "                            money_saved_demo[money_saved_demo.income_desc == '250K+'][categ])\n",
    "    \n",
    "    #Print statement\n",
    "    print_statement(money_saved_demo, results, 'income_desc', categ, show_details=True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_veg_debate_income = results_veg_debate_income.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_test(money_saved_demo, results_veg_debate_income, 'income_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the amounts of money saved thanks to campaigns by various groups (based on their age) for vegetarian and non-vegetarian products. Significant differences have only been found for vegetarian products (statistic=32.18, p-value=0.00071)\n",
    "    The post-hoc tests conducted using the Tukey's HSD method show significant differences between the following groups:\n",
    "    <ul>\n",
    "      <li>125-149K group saves more money on vegetarian products than the 15-24K group (difference of means: 5.1769, p-value=0.049)</li>\n",
    "      <li>75-99K group saves more money on vegetarian products than the 15-24K group (difference of means: 4.2839, p-value=0.024)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_veg_debate_age = pd.DataFrame(columns = ['Statistic', 'p-value', 'Reject'])\n",
    "for categ in veg_debate: \n",
    "    #Perform the Kruskal-Wallis test\n",
    "    results = stats.kruskal(money_saved_demo[money_saved_demo.age_desc == '19-24'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '25-34'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '35-44'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '45-54'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '55-64'][categ],\n",
    "                            money_saved_demo[money_saved_demo.age_desc == '65+'][categ])\n",
    "    \n",
    "    #Print statement\n",
    "    print_statement(money_saved_demo, results, 'age_desc', categ, show_details = True)\n",
    "    \n",
    "    #Store the results\n",
    "    results_veg_debate_age = results_veg_debate_age.append(store_results(results, categ), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_test(money_saved_demo, results_veg_debate_age, 'age_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FF4C2C;\">\n",
    "    <strong>Conclusion</strong> : We have conducted the Kruskal-Wallis test on the amounts of money saved thanks to campaigns by various groups (based on their age) for vegetarian and non-vegetarian products. Significant differences have only been found for vegetarian products (statistic=13.49, p-value=0.01921).\n",
    "    Unfortunately, post-hoc tests conducted using the Tukey's HSD method do not show any significant differences between the groups observed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
